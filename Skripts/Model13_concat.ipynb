{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow import keras\n",
    "import yaml\n",
    "#import tikzplotlib\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "# import Model13_FullyConvCodeDecode\n",
    "# importlib.reload(Model13_FullyConvCodeDecode)\n",
    "# from Model13_FullyConvCodeDecode import ConvModel\n",
    "\n",
    "import Model13_FullyConvCodeDecode2interpol\n",
    "importlib.reload(Model13_FullyConvCodeDecode2interpol)\n",
    "from Model13_FullyConvCodeDecode2interpol import ConvModel\n",
    "\n",
    "# import Model13_FullyConvCodeDecode3interpol\n",
    "# importlib.reload(Model13_FullyConvCodeDecode3interpol)\n",
    "# from Model13_FullyConvCodeDecode3interpol import ConvModel\n",
    "\n",
    "import seaborn as sns\n",
    "importlib.reload(sns)\n",
    "import Data_mdl13\n",
    "importlib.reload(Data_mdl13)\n",
    "from Data_mdl13 import DataProcessor_mdl13\n",
    "\n",
    "import Plots\n",
    "importlib.reload(Plots)\n",
    "from Plots import plot_CM, plot_CM_mplt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "The version_information extension is already loaded. To reload it, use:\n",
      "  %reload_ext version_information\n"
     ]
    }
   ],
   "source": [
    "cwd = Path.cwd()\n",
    "\n",
    "%load_ext tensorboard\n",
    "%load_ext version_information\n",
    "#!rm -rf ./logs/\n",
    "\n",
    "logsdir = cwd / '..' / 'logs'\n",
    "# !rmdir /s /q {logsdir}\n",
    "#!tensorboard --logdir {logsdir} --host localhost --port 6006\n",
    "\n",
    "cfg = 'config_mdl13_concat.yaml'\n",
    "with open(cfg, 'r') as f:\n",
    "    cfg_data = yaml.safe_load(f)\n",
    "seeds = cfg_data['seeds']\n",
    "folds = np.arange(1,21)\n",
    "\n",
    "folder = 'concat_sequences_fullyconv'\n",
    "datapath = cwd / '..' / 'Data' / 'DHG2016' / 'concat_sequences_fullyconv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permuting Samples using seed 26\n",
      "Make Windows and apply framreferences...\n",
      "Processing subject 1\n",
      "Processing subject 2\n"
     ]
    }
   ],
   "source": [
    "# process Data\n",
    "resultsDFslide = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "data_processor = DataProcessor_mdl13(cfg)\n",
    "data_processor.load_handgestdata()\n",
    "data_processor.handangles2windows()\n",
    "data_processor.save_config(folder)\n",
    "data_processor.save_windowsets(folder)\n",
    "\n",
    "\n",
    "\n",
    "foldsprocessors = dict()\n",
    "for fold in folds:\n",
    "    data_processor = DataProcessor_mdl13(cfg, fold = fold)\n",
    "    data_processor.load_windows(folder)\n",
    "    data_processor.processwindows()\n",
    "    data_processor.save_windowsets_processed(folder, name='Fold'+str(fold))\n",
    "\n",
    "# ohne die infos zu schreiben benötigt das Preprocessing 1 Minute statt 4 Minuten\n",
    "# 284 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating Fold 1\n",
      "training ...\n",
      "Setting seed to 5\n",
      "Epoch 1/30\n",
      "1226/1226 - 163s - 133ms/step - NLL: 1.1584 - accuracy: 0.6468 - loss: 1.3589 - precision: 0.8070 - recall: 0.5167 - val_NLL: 0.9569 - val_accuracy: 0.6930 - val_loss: 1.1395 - val_precision: 0.7978 - val_recall: 0.6168\n",
      "Epoch 2/30\n",
      "1226/1226 - 146s - 119ms/step - NLL: 0.7289 - accuracy: 0.7611 - loss: 0.9196 - precision: 0.8446 - recall: 0.6832 - val_NLL: 0.7495 - val_accuracy: 0.7473 - val_loss: 0.9498 - val_precision: 0.8217 - val_recall: 0.6961\n",
      "Epoch 3/30\n",
      "1226/1226 - 146s - 119ms/step - NLL: 0.5774 - accuracy: 0.8064 - loss: 0.7751 - precision: 0.8671 - recall: 0.7497 - val_NLL: 0.7219 - val_accuracy: 0.7642 - val_loss: 0.9212 - val_precision: 0.8184 - val_recall: 0.7270\n",
      "Epoch 4/30\n",
      "1226/1226 - 144s - 118ms/step - NLL: 0.5128 - accuracy: 0.8260 - loss: 0.7084 - precision: 0.8775 - recall: 0.7780 - val_NLL: 0.6744 - val_accuracy: 0.7803 - val_loss: 0.8721 - val_precision: 0.8279 - val_recall: 0.7473\n",
      "Epoch 5/30\n",
      "1226/1226 - 146s - 119ms/step - NLL: 0.4761 - accuracy: 0.8372 - loss: 0.6707 - precision: 0.8832 - recall: 0.7944 - val_NLL: 0.6773 - val_accuracy: 0.7740 - val_loss: 0.8739 - val_precision: 0.8229 - val_recall: 0.7395\n",
      "Epoch 6/30\n",
      "1226/1226 - 145s - 118ms/step - NLL: 0.4571 - accuracy: 0.8435 - loss: 0.6508 - precision: 0.8868 - recall: 0.8036 - val_NLL: 0.6652 - val_accuracy: 0.7798 - val_loss: 0.8615 - val_precision: 0.8238 - val_recall: 0.7506\n",
      "Epoch 7/30\n",
      "1226/1226 - 144s - 118ms/step - NLL: 0.4383 - accuracy: 0.8497 - loss: 0.6312 - precision: 0.8902 - recall: 0.8123 - val_NLL: 0.7177 - val_accuracy: 0.7705 - val_loss: 0.9125 - val_precision: 0.8120 - val_recall: 0.7440\n",
      "Epoch 8/30\n",
      "1226/1226 - 145s - 118ms/step - NLL: 0.4250 - accuracy: 0.8538 - loss: 0.6169 - precision: 0.8922 - recall: 0.8182 - val_NLL: 0.6868 - val_accuracy: 0.7805 - val_loss: 0.8803 - val_precision: 0.8197 - val_recall: 0.7555\n",
      "Epoch 9/30\n",
      "1226/1226 - 145s - 119ms/step - NLL: 0.4141 - accuracy: 0.8570 - loss: 0.6049 - precision: 0.8940 - recall: 0.8231 - val_NLL: 0.7139 - val_accuracy: 0.7683 - val_loss: 0.9071 - val_precision: 0.8130 - val_recall: 0.7414\n",
      "Epoch 10/30\n",
      "1226/1226 - 145s - 118ms/step - NLL: 0.4057 - accuracy: 0.8596 - loss: 0.5961 - precision: 0.8955 - recall: 0.8264 - val_NLL: 0.7024 - val_accuracy: 0.7740 - val_loss: 0.8955 - val_precision: 0.8135 - val_recall: 0.7461\n",
      "Epoch 11/30\n",
      "1226/1226 - 146s - 119ms/step - NLL: 0.3990 - accuracy: 0.8617 - loss: 0.5887 - precision: 0.8968 - recall: 0.8297 - val_NLL: 0.6909 - val_accuracy: 0.7761 - val_loss: 0.8835 - val_precision: 0.8169 - val_recall: 0.7469\n",
      "Epoch 12/30\n",
      "1226/1226 - 145s - 118ms/step - NLL: 0.3917 - accuracy: 0.8643 - loss: 0.5811 - precision: 0.8981 - recall: 0.8330 - val_NLL: 0.7696 - val_accuracy: 0.7660 - val_loss: 0.9651 - val_precision: 0.8057 - val_recall: 0.7409\n",
      "Epoch 13/30\n",
      "1226/1226 - 145s - 118ms/step - NLL: 0.3867 - accuracy: 0.8658 - loss: 0.5757 - precision: 0.8989 - recall: 0.8353 - val_NLL: 0.7233 - val_accuracy: 0.7710 - val_loss: 0.9171 - val_precision: 0.8119 - val_recall: 0.7433\n",
      "Epoch 14/30\n",
      "1226/1226 - 147s - 120ms/step - NLL: 0.3815 - accuracy: 0.8677 - loss: 0.5705 - precision: 0.9000 - recall: 0.8378 - val_NLL: 0.7046 - val_accuracy: 0.7779 - val_loss: 0.8991 - val_precision: 0.8147 - val_recall: 0.7544\n",
      "Epoch 15/30\n",
      "1226/1226 - 144s - 118ms/step - NLL: 0.3750 - accuracy: 0.8698 - loss: 0.5635 - precision: 0.9010 - recall: 0.8409 - val_NLL: 0.7200 - val_accuracy: 0.7719 - val_loss: 0.9123 - val_precision: 0.8112 - val_recall: 0.7458\n",
      "Epoch 16/30\n",
      "1226/1226 - 145s - 118ms/step - NLL: 0.3719 - accuracy: 0.8708 - loss: 0.5604 - precision: 0.9017 - recall: 0.8424 - val_NLL: 0.7026 - val_accuracy: 0.7776 - val_loss: 0.8954 - val_precision: 0.8138 - val_recall: 0.7527\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 57ms/step\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step\n",
      "NLL: 0.67\n",
      "Validation accuracy: 0.78\n",
      "Validation F1Score: 0.65\n",
      "evaluating Fold 2\n",
      "training ...\n",
      "Setting seed to 26\n",
      "Epoch 1/30\n",
      "1210/1210 - 152s - 125ms/step - NLL: 1.1342 - accuracy: 0.6544 - loss: 1.3473 - precision: 0.8095 - recall: 0.5273 - val_NLL: 1.0770 - val_accuracy: 0.6766 - val_loss: 1.2637 - val_precision: 0.7982 - val_recall: 0.6228\n",
      "Epoch 2/30\n",
      "1210/1210 - 145s - 120ms/step - NLL: 0.7098 - accuracy: 0.7670 - loss: 0.9039 - precision: 0.8478 - recall: 0.6918 - val_NLL: 1.0212 - val_accuracy: 0.7140 - val_loss: 1.2220 - val_precision: 0.7846 - val_recall: 0.6791\n",
      "Epoch 3/30\n",
      "1210/1210 - 144s - 119ms/step - NLL: 0.5583 - accuracy: 0.8125 - loss: 0.7582 - precision: 0.8706 - recall: 0.7582 - val_NLL: 0.9947 - val_accuracy: 0.7261 - val_loss: 1.1920 - val_precision: 0.7858 - val_recall: 0.6982\n",
      "Epoch 4/30\n",
      "1210/1210 - 145s - 120ms/step - NLL: 0.5013 - accuracy: 0.8297 - loss: 0.6985 - precision: 0.8793 - recall: 0.7836 - val_NLL: 0.9834 - val_accuracy: 0.7288 - val_loss: 1.1797 - val_precision: 0.7881 - val_recall: 0.6992\n",
      "Epoch 5/30\n",
      "1210/1210 - 144s - 119ms/step - NLL: 0.4685 - accuracy: 0.8400 - loss: 0.6646 - precision: 0.8846 - recall: 0.7987 - val_NLL: 0.9713 - val_accuracy: 0.7242 - val_loss: 1.1664 - val_precision: 0.7851 - val_recall: 0.6970\n",
      "Epoch 6/30\n",
      "1210/1210 - 147s - 122ms/step - NLL: 0.4462 - accuracy: 0.8471 - loss: 0.6411 - precision: 0.8883 - recall: 0.8088 - val_NLL: 1.0344 - val_accuracy: 0.7185 - val_loss: 1.2285 - val_precision: 0.7724 - val_recall: 0.6908\n",
      "Epoch 7/30\n",
      "1210/1210 - 143s - 118ms/step - NLL: 0.4302 - accuracy: 0.8522 - loss: 0.6248 - precision: 0.8909 - recall: 0.8167 - val_NLL: 1.0000 - val_accuracy: 0.7235 - val_loss: 1.1938 - val_precision: 0.7745 - val_recall: 0.6979\n",
      "Epoch 8/30\n",
      "1210/1210 - 142s - 117ms/step - NLL: 0.4179 - accuracy: 0.8564 - loss: 0.6120 - precision: 0.8927 - recall: 0.8225 - val_NLL: 1.0040 - val_accuracy: 0.7279 - val_loss: 1.1977 - val_precision: 0.7795 - val_recall: 0.7026\n",
      "Epoch 9/30\n",
      "1210/1210 - 142s - 118ms/step - NLL: 0.4086 - accuracy: 0.8591 - loss: 0.6024 - precision: 0.8940 - recall: 0.8269 - val_NLL: 1.0604 - val_accuracy: 0.7212 - val_loss: 1.2537 - val_precision: 0.7701 - val_recall: 0.6961\n",
      "Epoch 10/30\n",
      "1210/1210 - 143s - 118ms/step - NLL: 0.3981 - accuracy: 0.8628 - loss: 0.5915 - precision: 0.8962 - recall: 0.8318 - val_NLL: 1.0807 - val_accuracy: 0.7253 - val_loss: 1.2732 - val_precision: 0.7701 - val_recall: 0.7005\n",
      "Epoch 11/30\n",
      "1210/1210 - 143s - 118ms/step - NLL: 0.3897 - accuracy: 0.8656 - loss: 0.5822 - precision: 0.8977 - recall: 0.8358 - val_NLL: 1.0473 - val_accuracy: 0.7286 - val_loss: 1.2393 - val_precision: 0.7720 - val_recall: 0.7055\n",
      "Epoch 12/30\n",
      "1210/1210 - 143s - 118ms/step - NLL: 0.3821 - accuracy: 0.8683 - loss: 0.5745 - precision: 0.8992 - recall: 0.8395 - val_NLL: 1.0412 - val_accuracy: 0.7264 - val_loss: 1.2329 - val_precision: 0.7717 - val_recall: 0.7039\n",
      "Epoch 13/30\n",
      "1210/1210 - 143s - 119ms/step - NLL: 0.3761 - accuracy: 0.8703 - loss: 0.5683 - precision: 0.9002 - recall: 0.8424 - val_NLL: 1.0328 - val_accuracy: 0.7310 - val_loss: 1.2245 - val_precision: 0.7712 - val_recall: 0.7071\n",
      "Epoch 14/30\n",
      "1210/1210 - 143s - 118ms/step - NLL: 0.3698 - accuracy: 0.8722 - loss: 0.5617 - precision: 0.9011 - recall: 0.8453 - val_NLL: 1.0455 - val_accuracy: 0.7329 - val_loss: 1.2376 - val_precision: 0.7711 - val_recall: 0.7108\n",
      "Epoch 15/30\n",
      "1210/1210 - 143s - 118ms/step - NLL: 0.3644 - accuracy: 0.8743 - loss: 0.5563 - precision: 0.9024 - recall: 0.8481 - val_NLL: 1.0541 - val_accuracy: 0.7209 - val_loss: 1.2456 - val_precision: 0.7620 - val_recall: 0.6993\n",
      "\u001b[1m2420/2420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 35ms/step\n",
      "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step\n",
      "NLL: 0.97\n",
      "Validation accuracy: 0.72\n",
      "Validation F1Score: 0.45\n",
      "evaluating Fold 3\n",
      "training ...\n",
      "Setting seed to 90\n",
      "Epoch 1/30\n",
      "1218/1218 - 149s - 122ms/step - NLL: 1.1773 - accuracy: 0.6445 - loss: 1.3725 - precision: 0.8098 - recall: 0.5163 - val_NLL: 0.9535 - val_accuracy: 0.7047 - val_loss: 1.1345 - val_precision: 0.8085 - val_recall: 0.6328\n",
      "Epoch 2/30\n",
      "1218/1218 - 148s - 122ms/step - NLL: 0.7067 - accuracy: 0.7661 - loss: 0.8988 - precision: 0.8499 - recall: 0.6919 - val_NLL: 0.6819 - val_accuracy: 0.7807 - val_loss: 0.8787 - val_precision: 0.8368 - val_recall: 0.7334\n",
      "Epoch 3/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.5678 - accuracy: 0.8066 - loss: 0.7648 - precision: 0.8698 - recall: 0.7499 - val_NLL: 0.6186 - val_accuracy: 0.8002 - val_loss: 0.8154 - val_precision: 0.8470 - val_recall: 0.7607\n",
      "Epoch 4/30\n",
      "1218/1218 - 142s - 116ms/step - NLL: 0.5100 - accuracy: 0.8244 - loss: 0.7063 - precision: 0.8787 - recall: 0.7755 - val_NLL: 0.6063 - val_accuracy: 0.8053 - val_loss: 0.8017 - val_precision: 0.8451 - val_recall: 0.7701\n",
      "Epoch 5/30\n",
      "1218/1218 - 145s - 119ms/step - NLL: 0.4789 - accuracy: 0.8348 - loss: 0.6744 - precision: 0.8839 - recall: 0.7901 - val_NLL: 0.5862 - val_accuracy: 0.8056 - val_loss: 0.7810 - val_precision: 0.8465 - val_recall: 0.7703\n",
      "Epoch 6/30\n",
      "1218/1218 - 146s - 120ms/step - NLL: 0.4565 - accuracy: 0.8422 - loss: 0.6519 - precision: 0.8870 - recall: 0.8008 - val_NLL: 0.5990 - val_accuracy: 0.8095 - val_loss: 0.7939 - val_precision: 0.8436 - val_recall: 0.7818\n",
      "Epoch 7/30\n",
      "1218/1218 - 142s - 116ms/step - NLL: 0.4405 - accuracy: 0.8473 - loss: 0.6357 - precision: 0.8896 - recall: 0.8088 - val_NLL: 0.6262 - val_accuracy: 0.8005 - val_loss: 0.8212 - val_precision: 0.8325 - val_recall: 0.7752\n",
      "Epoch 8/30\n",
      "1218/1218 - 144s - 118ms/step - NLL: 0.4257 - accuracy: 0.8527 - loss: 0.6205 - precision: 0.8922 - recall: 0.8162 - val_NLL: 0.6333 - val_accuracy: 0.8060 - val_loss: 0.8279 - val_precision: 0.8351 - val_recall: 0.7832\n",
      "Epoch 9/30\n",
      "1218/1218 - 143s - 117ms/step - NLL: 0.4145 - accuracy: 0.8563 - loss: 0.6092 - precision: 0.8942 - recall: 0.8217 - val_NLL: 0.6579 - val_accuracy: 0.7998 - val_loss: 0.8526 - val_precision: 0.8256 - val_recall: 0.7818\n",
      "Epoch 10/30\n",
      "1218/1218 - 141s - 115ms/step - NLL: 0.4057 - accuracy: 0.8594 - loss: 0.6003 - precision: 0.8955 - recall: 0.8262 - val_NLL: 0.5812 - val_accuracy: 0.8126 - val_loss: 0.7759 - val_precision: 0.8417 - val_recall: 0.7897\n",
      "Epoch 11/30\n",
      "1218/1218 - 141s - 116ms/step - NLL: 0.3972 - accuracy: 0.8621 - loss: 0.5914 - precision: 0.8968 - recall: 0.8302 - val_NLL: 0.6176 - val_accuracy: 0.8056 - val_loss: 0.8120 - val_precision: 0.8294 - val_recall: 0.7855\n",
      "Epoch 12/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.3887 - accuracy: 0.8653 - loss: 0.5828 - precision: 0.8985 - recall: 0.8346 - val_NLL: 0.6307 - val_accuracy: 0.8071 - val_loss: 0.8252 - val_precision: 0.8321 - val_recall: 0.7864\n",
      "Epoch 13/30\n",
      "1218/1218 - 141s - 116ms/step - NLL: 0.3825 - accuracy: 0.8674 - loss: 0.5766 - precision: 0.8994 - recall: 0.8379 - val_NLL: 0.6357 - val_accuracy: 0.8067 - val_loss: 0.8300 - val_precision: 0.8301 - val_recall: 0.7881\n",
      "Epoch 14/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.3769 - accuracy: 0.8695 - loss: 0.5711 - precision: 0.9004 - recall: 0.8410 - val_NLL: 0.6589 - val_accuracy: 0.8110 - val_loss: 0.8532 - val_precision: 0.8372 - val_recall: 0.7925\n",
      "Epoch 15/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.3696 - accuracy: 0.8722 - loss: 0.5640 - precision: 0.9019 - recall: 0.8450 - val_NLL: 0.6690 - val_accuracy: 0.8056 - val_loss: 0.8638 - val_precision: 0.8290 - val_recall: 0.7866\n",
      "Epoch 16/30\n",
      "1218/1218 - 141s - 116ms/step - NLL: 0.3646 - accuracy: 0.8740 - loss: 0.5593 - precision: 0.9026 - recall: 0.8475 - val_NLL: 0.6814 - val_accuracy: 0.8033 - val_loss: 0.8758 - val_precision: 0.8286 - val_recall: 0.7826\n",
      "Epoch 17/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.3583 - accuracy: 0.8763 - loss: 0.5526 - precision: 0.9041 - recall: 0.8507 - val_NLL: 0.6702 - val_accuracy: 0.8042 - val_loss: 0.8656 - val_precision: 0.8279 - val_recall: 0.7857\n",
      "Epoch 18/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.3531 - accuracy: 0.8783 - loss: 0.5476 - precision: 0.9053 - recall: 0.8534 - val_NLL: 0.6803 - val_accuracy: 0.8094 - val_loss: 0.8761 - val_precision: 0.8325 - val_recall: 0.7917\n",
      "Epoch 19/30\n",
      "1218/1218 - 142s - 116ms/step - NLL: 0.3485 - accuracy: 0.8797 - loss: 0.5434 - precision: 0.9060 - recall: 0.8555 - val_NLL: 0.6504 - val_accuracy: 0.8154 - val_loss: 0.8456 - val_precision: 0.8365 - val_recall: 0.7986\n",
      "Epoch 20/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.3452 - accuracy: 0.8809 - loss: 0.5402 - precision: 0.9065 - recall: 0.8574 - val_NLL: 0.6604 - val_accuracy: 0.8130 - val_loss: 0.8563 - val_precision: 0.8356 - val_recall: 0.7938\n",
      "\u001b[1m2436/2436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 33ms/step\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step\n",
      "NLL: 0.58\n",
      "Validation accuracy: 0.81\n",
      "Validation F1Score: 0.72\n",
      "evaluating Fold 4\n",
      "training ...\n",
      "Setting seed to 232\n",
      "Epoch 1/30\n",
      "1223/1223 - 155s - 127ms/step - NLL: 1.1006 - accuracy: 0.6631 - loss: 1.3243 - precision: 0.8102 - recall: 0.5418 - val_NLL: 0.7230 - val_accuracy: 0.7728 - val_loss: 0.9215 - val_precision: 0.8616 - val_recall: 0.6847\n",
      "Epoch 2/30\n",
      "1223/1223 - 148s - 121ms/step - NLL: 0.6743 - accuracy: 0.7771 - loss: 0.8755 - precision: 0.8521 - recall: 0.7079 - val_NLL: 0.6316 - val_accuracy: 0.7892 - val_loss: 0.8347 - val_precision: 0.8431 - val_recall: 0.7484\n",
      "Epoch 3/30\n",
      "1223/1223 - 144s - 117ms/step - NLL: 0.5498 - accuracy: 0.8140 - loss: 0.7525 - precision: 0.8712 - recall: 0.7609 - val_NLL: 0.6180 - val_accuracy: 0.7959 - val_loss: 0.8196 - val_precision: 0.8415 - val_recall: 0.7622\n",
      "Epoch 4/30\n",
      "1223/1223 - 144s - 118ms/step - NLL: 0.4956 - accuracy: 0.8303 - loss: 0.6956 - precision: 0.8793 - recall: 0.7850 - val_NLL: 0.5855 - val_accuracy: 0.8095 - val_loss: 0.7859 - val_precision: 0.8508 - val_recall: 0.7762\n",
      "Epoch 5/30\n",
      "1223/1223 - 201s - 164ms/step - NLL: 0.4645 - accuracy: 0.8405 - loss: 0.6630 - precision: 0.8848 - recall: 0.7991 - val_NLL: 0.5815 - val_accuracy: 0.8105 - val_loss: 0.7808 - val_precision: 0.8499 - val_recall: 0.7811\n",
      "Epoch 6/30\n",
      "1223/1223 - 144s - 118ms/step - NLL: 0.4443 - accuracy: 0.8467 - loss: 0.6420 - precision: 0.8881 - recall: 0.8083 - val_NLL: 0.5619 - val_accuracy: 0.8127 - val_loss: 0.7612 - val_precision: 0.8475 - val_recall: 0.7871\n",
      "Epoch 7/30\n",
      "1223/1223 - 145s - 119ms/step - NLL: 0.4282 - accuracy: 0.8521 - loss: 0.6251 - precision: 0.8910 - recall: 0.8159 - val_NLL: 0.6138 - val_accuracy: 0.8047 - val_loss: 0.8103 - val_precision: 0.8394 - val_recall: 0.7798\n",
      "Epoch 8/30\n",
      "1223/1223 - 148s - 121ms/step - NLL: 0.4158 - accuracy: 0.8561 - loss: 0.6117 - precision: 0.8929 - recall: 0.8219 - val_NLL: 0.6299 - val_accuracy: 0.7980 - val_loss: 0.8268 - val_precision: 0.8352 - val_recall: 0.7727\n",
      "Epoch 9/30\n",
      "1223/1223 - 144s - 118ms/step - NLL: 0.4064 - accuracy: 0.8590 - loss: 0.6025 - precision: 0.8940 - recall: 0.8264 - val_NLL: 0.5852 - val_accuracy: 0.8104 - val_loss: 0.7805 - val_precision: 0.8465 - val_recall: 0.7845\n",
      "Epoch 10/30\n",
      "1223/1223 - 145s - 119ms/step - NLL: 0.3950 - accuracy: 0.8631 - loss: 0.5910 - precision: 0.8962 - recall: 0.8320 - val_NLL: 0.6334 - val_accuracy: 0.7996 - val_loss: 0.8295 - val_precision: 0.8361 - val_recall: 0.7745\n",
      "Epoch 11/30\n",
      "1223/1223 - 145s - 118ms/step - NLL: 0.3872 - accuracy: 0.8658 - loss: 0.5825 - precision: 0.8978 - recall: 0.8357 - val_NLL: 0.6113 - val_accuracy: 0.8101 - val_loss: 0.8065 - val_precision: 0.8420 - val_recall: 0.7863\n",
      "Epoch 12/30\n",
      "1223/1223 - 145s - 119ms/step - NLL: 0.3803 - accuracy: 0.8684 - loss: 0.5756 - precision: 0.8991 - recall: 0.8394 - val_NLL: 0.6389 - val_accuracy: 0.8057 - val_loss: 0.8348 - val_precision: 0.8359 - val_recall: 0.7826\n",
      "Epoch 13/30\n",
      "1223/1223 - 144s - 118ms/step - NLL: 0.3753 - accuracy: 0.8699 - loss: 0.5712 - precision: 0.8999 - recall: 0.8416 - val_NLL: 0.6227 - val_accuracy: 0.8050 - val_loss: 0.8204 - val_precision: 0.8362 - val_recall: 0.7829\n",
      "Epoch 14/30\n",
      "1223/1223 - 146s - 119ms/step - NLL: 0.3657 - accuracy: 0.8733 - loss: 0.5616 - precision: 0.9019 - recall: 0.8465 - val_NLL: 0.6339 - val_accuracy: 0.8055 - val_loss: 0.8304 - val_precision: 0.8388 - val_recall: 0.7820\n",
      "Epoch 15/30\n",
      "1223/1223 - 148s - 121ms/step - NLL: 0.3613 - accuracy: 0.8751 - loss: 0.5575 - precision: 0.9029 - recall: 0.8489 - val_NLL: 0.6911 - val_accuracy: 0.7992 - val_loss: 0.8872 - val_precision: 0.8272 - val_recall: 0.7792\n",
      "Epoch 16/30\n",
      "1223/1223 - 143s - 117ms/step - NLL: 0.3561 - accuracy: 0.8768 - loss: 0.5522 - precision: 0.9037 - recall: 0.8513 - val_NLL: 0.6920 - val_accuracy: 0.7935 - val_loss: 0.8893 - val_precision: 0.8209 - val_recall: 0.7741\n",
      "\u001b[1m2445/2445\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 34ms/step\n",
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step\n",
      "NLL: 0.56\n",
      "Validation accuracy: 0.81\n",
      "Validation F1Score: 0.65\n",
      "evaluating Fold 5\n",
      "training ...\n",
      "Setting seed to 819\n",
      "Epoch 1/30\n",
      "1221/1221 - 169s - 138ms/step - NLL: 1.1492 - accuracy: 0.6499 - loss: 1.3554 - precision: 0.8112 - recall: 0.5254 - val_NLL: 0.8019 - val_accuracy: 0.7485 - val_loss: 0.9890 - val_precision: 0.8377 - val_recall: 0.6488\n",
      "Epoch 2/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.7047 - accuracy: 0.7670 - loss: 0.8996 - precision: 0.8497 - recall: 0.6929 - val_NLL: 0.7259 - val_accuracy: 0.7717 - val_loss: 0.9247 - val_precision: 0.8284 - val_recall: 0.7176\n",
      "Epoch 3/30\n",
      "1221/1221 - 146s - 119ms/step - NLL: 0.5601 - accuracy: 0.8091 - loss: 0.7598 - precision: 0.8696 - recall: 0.7545 - val_NLL: 0.6874 - val_accuracy: 0.7903 - val_loss: 0.8864 - val_precision: 0.8303 - val_recall: 0.7532\n",
      "Epoch 4/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.4973 - accuracy: 0.8290 - loss: 0.6946 - precision: 0.8800 - recall: 0.7828 - val_NLL: 0.6572 - val_accuracy: 0.8016 - val_loss: 0.8536 - val_precision: 0.8359 - val_recall: 0.7682\n",
      "Epoch 5/30\n",
      "1221/1221 - 146s - 120ms/step - NLL: 0.4672 - accuracy: 0.8383 - loss: 0.6629 - precision: 0.8847 - recall: 0.7970 - val_NLL: 0.6311 - val_accuracy: 0.8067 - val_loss: 0.8250 - val_precision: 0.8426 - val_recall: 0.7726\n",
      "Epoch 6/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.4458 - accuracy: 0.8456 - loss: 0.6406 - precision: 0.8884 - recall: 0.8071 - val_NLL: 0.6527 - val_accuracy: 0.8018 - val_loss: 0.8470 - val_precision: 0.8347 - val_recall: 0.7698\n",
      "Epoch 7/30\n",
      "1221/1221 - 145s - 118ms/step - NLL: 0.4305 - accuracy: 0.8506 - loss: 0.6248 - precision: 0.8912 - recall: 0.8143 - val_NLL: 0.6178 - val_accuracy: 0.8073 - val_loss: 0.8112 - val_precision: 0.8384 - val_recall: 0.7757\n",
      "Epoch 8/30\n",
      "1221/1221 - 146s - 120ms/step - NLL: 0.4172 - accuracy: 0.8551 - loss: 0.6106 - precision: 0.8934 - recall: 0.8207 - val_NLL: 0.6226 - val_accuracy: 0.8090 - val_loss: 0.8149 - val_precision: 0.8421 - val_recall: 0.7798\n",
      "Epoch 9/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.4082 - accuracy: 0.8582 - loss: 0.6009 - precision: 0.8949 - recall: 0.8252 - val_NLL: 0.6406 - val_accuracy: 0.8064 - val_loss: 0.8317 - val_precision: 0.8351 - val_recall: 0.7813\n",
      "Epoch 10/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.4017 - accuracy: 0.8606 - loss: 0.5935 - precision: 0.8958 - recall: 0.8286 - val_NLL: 0.6595 - val_accuracy: 0.7997 - val_loss: 0.8503 - val_precision: 0.8337 - val_recall: 0.7740\n",
      "Epoch 11/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.3942 - accuracy: 0.8632 - loss: 0.5864 - precision: 0.8973 - recall: 0.8322 - val_NLL: 0.6589 - val_accuracy: 0.8045 - val_loss: 0.8500 - val_precision: 0.8330 - val_recall: 0.7808\n",
      "Epoch 12/30\n",
      "1221/1221 - 146s - 119ms/step - NLL: 0.3875 - accuracy: 0.8653 - loss: 0.5791 - precision: 0.8983 - recall: 0.8354 - val_NLL: 0.6628 - val_accuracy: 0.8042 - val_loss: 0.8533 - val_precision: 0.8339 - val_recall: 0.7813\n",
      "Epoch 13/30\n",
      "1221/1221 - 144s - 118ms/step - NLL: 0.3825 - accuracy: 0.8670 - loss: 0.5739 - precision: 0.8990 - recall: 0.8380 - val_NLL: 0.6592 - val_accuracy: 0.8030 - val_loss: 0.8495 - val_precision: 0.8347 - val_recall: 0.7794\n",
      "Epoch 14/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.3768 - accuracy: 0.8691 - loss: 0.5682 - precision: 0.9002 - recall: 0.8410 - val_NLL: 0.6377 - val_accuracy: 0.8068 - val_loss: 0.8280 - val_precision: 0.8371 - val_recall: 0.7830\n",
      "Epoch 15/30\n",
      "1221/1221 - 146s - 120ms/step - NLL: 0.3722 - accuracy: 0.8705 - loss: 0.5634 - precision: 0.9008 - recall: 0.8428 - val_NLL: 0.6524 - val_accuracy: 0.8080 - val_loss: 0.8427 - val_precision: 0.8372 - val_recall: 0.7863\n",
      "Epoch 16/30\n",
      "1221/1221 - 146s - 119ms/step - NLL: 0.3676 - accuracy: 0.8724 - loss: 0.5587 - precision: 0.9020 - recall: 0.8457 - val_NLL: 0.6356 - val_accuracy: 0.8043 - val_loss: 0.8256 - val_precision: 0.8360 - val_recall: 0.7807\n",
      "Epoch 17/30\n",
      "1221/1221 - 145s - 119ms/step - NLL: 0.3633 - accuracy: 0.8741 - loss: 0.5542 - precision: 0.9029 - recall: 0.8479 - val_NLL: 0.6389 - val_accuracy: 0.8053 - val_loss: 0.8288 - val_precision: 0.8355 - val_recall: 0.7827\n",
      "\u001b[1m2442/2442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 34ms/step\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step\n",
      "NLL: 0.62\n",
      "Validation accuracy: 0.81\n",
      "Validation F1Score: 0.66\n",
      "evaluating Fold 6\n",
      "training ...\n",
      "Setting seed to 1027\n",
      "Epoch 1/30\n",
      "1194/1194 - 168s - 141ms/step - NLL: 1.2094 - accuracy: 0.6375 - loss: 1.4302 - precision: 0.8109 - recall: 0.5000 - val_NLL: 0.9881 - val_accuracy: 0.7183 - val_loss: 1.1734 - val_precision: 0.8151 - val_recall: 0.6350\n",
      "Epoch 2/30\n",
      "1194/1194 - 144s - 120ms/step - NLL: 0.7432 - accuracy: 0.7557 - loss: 0.9379 - precision: 0.8430 - recall: 0.6755 - val_NLL: 0.7683 - val_accuracy: 0.7699 - val_loss: 0.9758 - val_precision: 0.8448 - val_recall: 0.7206\n",
      "Epoch 3/30\n",
      "1194/1194 - 144s - 121ms/step - NLL: 0.5616 - accuracy: 0.8097 - loss: 0.7633 - precision: 0.8680 - recall: 0.7552 - val_NLL: 0.6883 - val_accuracy: 0.7953 - val_loss: 0.8999 - val_precision: 0.8552 - val_recall: 0.7504\n",
      "Epoch 4/30\n",
      "1194/1194 - 145s - 122ms/step - NLL: 0.4978 - accuracy: 0.8295 - loss: 0.6981 - precision: 0.8779 - recall: 0.7846 - val_NLL: 0.6634 - val_accuracy: 0.8019 - val_loss: 0.8751 - val_precision: 0.8540 - val_recall: 0.7673\n",
      "Epoch 5/30\n",
      "1194/1194 - 145s - 121ms/step - NLL: 0.4637 - accuracy: 0.8399 - loss: 0.6625 - precision: 0.8831 - recall: 0.7999 - val_NLL: 0.6724 - val_accuracy: 0.7958 - val_loss: 0.8844 - val_precision: 0.8556 - val_recall: 0.7520\n",
      "Epoch 6/30\n",
      "1194/1194 - 145s - 121ms/step - NLL: 0.4400 - accuracy: 0.8478 - loss: 0.6376 - precision: 0.8877 - recall: 0.8113 - val_NLL: 0.6570 - val_accuracy: 0.7993 - val_loss: 0.8632 - val_precision: 0.8476 - val_recall: 0.7672\n",
      "Epoch 7/30\n",
      "1194/1194 - 144s - 120ms/step - NLL: 0.4223 - accuracy: 0.8535 - loss: 0.6197 - precision: 0.8905 - recall: 0.8195 - val_NLL: 0.7250 - val_accuracy: 0.7878 - val_loss: 0.9350 - val_precision: 0.8393 - val_recall: 0.7550\n",
      "Epoch 8/30\n",
      "1194/1194 - 144s - 120ms/step - NLL: 0.4093 - accuracy: 0.8578 - loss: 0.6062 - precision: 0.8927 - recall: 0.8256 - val_NLL: 0.7723 - val_accuracy: 0.7885 - val_loss: 0.9879 - val_precision: 0.8313 - val_recall: 0.7619\n",
      "Epoch 9/30\n",
      "1194/1194 - 145s - 122ms/step - NLL: 0.3992 - accuracy: 0.8613 - loss: 0.5962 - precision: 0.8944 - recall: 0.8309 - val_NLL: 0.7409 - val_accuracy: 0.7903 - val_loss: 0.9564 - val_precision: 0.8331 - val_recall: 0.7617\n",
      "Epoch 10/30\n",
      "1194/1194 - 146s - 122ms/step - NLL: 0.3890 - accuracy: 0.8650 - loss: 0.5856 - precision: 0.8962 - recall: 0.8361 - val_NLL: 0.7533 - val_accuracy: 0.7872 - val_loss: 0.9697 - val_precision: 0.8286 - val_recall: 0.7608\n",
      "Epoch 11/30\n",
      "1194/1194 - 144s - 121ms/step - NLL: 0.3810 - accuracy: 0.8677 - loss: 0.5773 - precision: 0.8980 - recall: 0.8401 - val_NLL: 0.7442 - val_accuracy: 0.7924 - val_loss: 0.9499 - val_precision: 0.8257 - val_recall: 0.7687\n",
      "Epoch 12/30\n",
      "1194/1194 - 145s - 121ms/step - NLL: 0.3742 - accuracy: 0.8701 - loss: 0.5703 - precision: 0.8994 - recall: 0.8432 - val_NLL: 0.7510 - val_accuracy: 0.7913 - val_loss: 0.9615 - val_precision: 0.8266 - val_recall: 0.7680\n",
      "Epoch 13/30\n",
      "1194/1194 - 144s - 121ms/step - NLL: 0.3659 - accuracy: 0.8731 - loss: 0.5620 - precision: 0.9011 - recall: 0.8473 - val_NLL: 0.7165 - val_accuracy: 0.7984 - val_loss: 0.9193 - val_precision: 0.8328 - val_recall: 0.7740\n",
      "Epoch 14/30\n",
      "1194/1194 - 145s - 122ms/step - NLL: 0.3618 - accuracy: 0.8744 - loss: 0.5580 - precision: 0.9019 - recall: 0.8492 - val_NLL: 0.7637 - val_accuracy: 0.7880 - val_loss: 0.9762 - val_precision: 0.8228 - val_recall: 0.7639\n",
      "Epoch 15/30\n",
      "1194/1194 - 144s - 121ms/step - NLL: 0.3542 - accuracy: 0.8769 - loss: 0.5494 - precision: 0.9031 - recall: 0.8526 - val_NLL: 0.8456 - val_accuracy: 0.7752 - val_loss: 1.0555 - val_precision: 0.8054 - val_recall: 0.7549\n",
      "Epoch 16/30\n",
      "1194/1194 - 145s - 121ms/step - NLL: 0.3494 - accuracy: 0.8789 - loss: 0.5443 - precision: 0.9046 - recall: 0.8551 - val_NLL: 0.8231 - val_accuracy: 0.7828 - val_loss: 1.0357 - val_precision: 0.8127 - val_recall: 0.7629\n",
      "\u001b[1m2387/2387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 35ms/step\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step\n",
      "NLL: 0.66\n",
      "Validation accuracy: 0.80\n",
      "Validation F1Score: 0.60\n",
      "evaluating Fold 7\n",
      "training ...\n",
      "Setting seed to 5825\n",
      "Epoch 1/30\n",
      "1217/1217 - 157s - 129ms/step - NLL: 1.1721 - accuracy: 0.6466 - loss: 1.3800 - precision: 0.8132 - recall: 0.5186 - val_NLL: 0.9347 - val_accuracy: 0.6982 - val_loss: 1.1158 - val_precision: 0.8103 - val_recall: 0.6047\n",
      "Epoch 2/30\n",
      "1217/1217 - 147s - 120ms/step - NLL: 0.7320 - accuracy: 0.7603 - loss: 0.9227 - precision: 0.8466 - recall: 0.6823 - val_NLL: 0.8063 - val_accuracy: 0.7415 - val_loss: 1.0021 - val_precision: 0.8176 - val_recall: 0.6789\n",
      "Epoch 3/30\n",
      "1217/1217 - 149s - 122ms/step - NLL: 0.5810 - accuracy: 0.8055 - loss: 0.7800 - precision: 0.8680 - recall: 0.7475 - val_NLL: 0.7733 - val_accuracy: 0.7521 - val_loss: 0.9741 - val_precision: 0.8159 - val_recall: 0.7014\n",
      "Epoch 4/30\n",
      "1217/1217 - 147s - 121ms/step - NLL: 0.5100 - accuracy: 0.8275 - loss: 0.7116 - precision: 0.8792 - recall: 0.7797 - val_NLL: 0.6994 - val_accuracy: 0.7670 - val_loss: 0.8995 - val_precision: 0.8170 - val_recall: 0.7250\n",
      "Epoch 5/30\n",
      "1217/1217 - 148s - 122ms/step - NLL: 0.4734 - accuracy: 0.8389 - loss: 0.6744 - precision: 0.8844 - recall: 0.7966 - val_NLL: 0.7348 - val_accuracy: 0.7602 - val_loss: 0.9347 - val_precision: 0.8092 - val_recall: 0.7215\n",
      "Epoch 6/30\n",
      "1217/1217 - 147s - 121ms/step - NLL: 0.4458 - accuracy: 0.8469 - loss: 0.6450 - precision: 0.8879 - recall: 0.8092 - val_NLL: 0.6904 - val_accuracy: 0.7665 - val_loss: 0.8887 - val_precision: 0.8093 - val_recall: 0.7296\n",
      "Epoch 7/30\n",
      "1217/1217 - 149s - 122ms/step - NLL: 0.4268 - accuracy: 0.8529 - loss: 0.6246 - precision: 0.8910 - recall: 0.8179 - val_NLL: 0.7046 - val_accuracy: 0.7679 - val_loss: 0.9019 - val_precision: 0.8102 - val_recall: 0.7333\n",
      "Epoch 8/30\n",
      "1217/1217 - 146s - 120ms/step - NLL: 0.4143 - accuracy: 0.8569 - loss: 0.6116 - precision: 0.8929 - recall: 0.8240 - val_NLL: 0.6827 - val_accuracy: 0.7680 - val_loss: 0.8791 - val_precision: 0.8096 - val_recall: 0.7323\n",
      "Epoch 9/30\n",
      "1217/1217 - 148s - 122ms/step - NLL: 0.4031 - accuracy: 0.8604 - loss: 0.5996 - precision: 0.8949 - recall: 0.8288 - val_NLL: 0.7382 - val_accuracy: 0.7640 - val_loss: 0.9335 - val_precision: 0.8030 - val_recall: 0.7297\n",
      "Epoch 10/30\n",
      "1217/1217 - 146s - 120ms/step - NLL: 0.3936 - accuracy: 0.8637 - loss: 0.5894 - precision: 0.8968 - recall: 0.8333 - val_NLL: 0.7002 - val_accuracy: 0.7683 - val_loss: 0.8949 - val_precision: 0.8111 - val_recall: 0.7337\n",
      "Epoch 11/30\n",
      "1217/1217 - 148s - 122ms/step - NLL: 0.3864 - accuracy: 0.8662 - loss: 0.5815 - precision: 0.8980 - recall: 0.8368 - val_NLL: 0.7747 - val_accuracy: 0.7558 - val_loss: 0.9691 - val_precision: 0.7953 - val_recall: 0.7228\n",
      "Epoch 12/30\n",
      "1217/1217 - 146s - 120ms/step - NLL: 0.3787 - accuracy: 0.8687 - loss: 0.5732 - precision: 0.8994 - recall: 0.8402 - val_NLL: 0.7352 - val_accuracy: 0.7675 - val_loss: 0.9298 - val_precision: 0.8067 - val_recall: 0.7324\n",
      "Epoch 13/30\n",
      "1217/1217 - 148s - 122ms/step - NLL: 0.3710 - accuracy: 0.8714 - loss: 0.5648 - precision: 0.9009 - recall: 0.8440 - val_NLL: 0.7735 - val_accuracy: 0.7612 - val_loss: 0.9671 - val_precision: 0.7984 - val_recall: 0.7300\n",
      "Epoch 14/30\n",
      "1217/1217 - 145s - 119ms/step - NLL: 0.3650 - accuracy: 0.8736 - loss: 0.5583 - precision: 0.9022 - recall: 0.8472 - val_NLL: 0.7510 - val_accuracy: 0.7648 - val_loss: 0.9433 - val_precision: 0.7998 - val_recall: 0.7341\n",
      "Epoch 15/30\n",
      "1217/1217 - 149s - 122ms/step - NLL: 0.3593 - accuracy: 0.8754 - loss: 0.5522 - precision: 0.9031 - recall: 0.8496 - val_NLL: 0.7597 - val_accuracy: 0.7651 - val_loss: 0.9520 - val_precision: 0.7986 - val_recall: 0.7379\n",
      "Epoch 16/30\n",
      "1217/1217 - 147s - 121ms/step - NLL: 0.3544 - accuracy: 0.8772 - loss: 0.5469 - precision: 0.9041 - recall: 0.8522 - val_NLL: 0.7778 - val_accuracy: 0.7605 - val_loss: 0.9689 - val_precision: 0.7924 - val_recall: 0.7282\n",
      "Epoch 17/30\n",
      "1217/1217 - 150s - 123ms/step - NLL: 0.3493 - accuracy: 0.8789 - loss: 0.5415 - precision: 0.9052 - recall: 0.8545 - val_NLL: 0.7438 - val_accuracy: 0.7714 - val_loss: 0.9350 - val_precision: 0.8049 - val_recall: 0.7453\n",
      "Epoch 18/30\n",
      "1217/1217 - 147s - 121ms/step - NLL: 0.3438 - accuracy: 0.8808 - loss: 0.5359 - precision: 0.9061 - recall: 0.8571 - val_NLL: 0.7614 - val_accuracy: 0.7693 - val_loss: 0.9525 - val_precision: 0.8053 - val_recall: 0.7384\n",
      "\u001b[1m2433/2433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 34ms/step\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step\n",
      "NLL: 0.68\n",
      "Validation accuracy: 0.77\n",
      "Validation F1Score: 0.65\n",
      "evaluating Fold 8\n",
      "training ...\n",
      "Setting seed to 52408\n",
      "Epoch 1/30\n",
      "1215/1215 - 168s - 138ms/step - NLL: 1.1411 - accuracy: 0.6540 - loss: 1.3432 - precision: 0.8125 - recall: 0.5303 - val_NLL: 0.7427 - val_accuracy: 0.7504 - val_loss: 0.9267 - val_precision: 0.8624 - val_recall: 0.6616\n",
      "Epoch 2/30\n",
      "1215/1215 - 149s - 123ms/step - NLL: 0.6843 - accuracy: 0.7740 - loss: 0.8795 - precision: 0.8497 - recall: 0.7039 - val_NLL: 0.6004 - val_accuracy: 0.7965 - val_loss: 0.7991 - val_precision: 0.8626 - val_recall: 0.7468\n",
      "Epoch 3/30\n",
      "1215/1215 - 148s - 122ms/step - NLL: 0.5478 - accuracy: 0.8145 - loss: 0.7483 - precision: 0.8704 - recall: 0.7634 - val_NLL: 0.5529 - val_accuracy: 0.8029 - val_loss: 0.7515 - val_precision: 0.8622 - val_recall: 0.7633\n",
      "Epoch 4/30\n",
      "1215/1215 - 146s - 121ms/step - NLL: 0.4901 - accuracy: 0.8322 - loss: 0.6886 - precision: 0.8792 - recall: 0.7892 - val_NLL: 0.5405 - val_accuracy: 0.8079 - val_loss: 0.7365 - val_precision: 0.8629 - val_recall: 0.7711\n",
      "Epoch 5/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.4596 - accuracy: 0.8416 - loss: 0.6572 - precision: 0.8840 - recall: 0.8028 - val_NLL: 0.4963 - val_accuracy: 0.8244 - val_loss: 0.6924 - val_precision: 0.8746 - val_recall: 0.7906\n",
      "Epoch 6/30\n",
      "1215/1215 - 145s - 119ms/step - NLL: 0.4389 - accuracy: 0.8481 - loss: 0.6351 - precision: 0.8872 - recall: 0.8124 - val_NLL: 0.5020 - val_accuracy: 0.8195 - val_loss: 0.6969 - val_precision: 0.8670 - val_recall: 0.7893\n",
      "Epoch 7/30\n",
      "1215/1215 - 145s - 119ms/step - NLL: 0.4228 - accuracy: 0.8532 - loss: 0.6187 - precision: 0.8899 - recall: 0.8199 - val_NLL: 0.4949 - val_accuracy: 0.8234 - val_loss: 0.6892 - val_precision: 0.8684 - val_recall: 0.7930\n",
      "Epoch 8/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.4115 - accuracy: 0.8570 - loss: 0.6074 - precision: 0.8919 - recall: 0.8252 - val_NLL: 0.5070 - val_accuracy: 0.8257 - val_loss: 0.7031 - val_precision: 0.8671 - val_recall: 0.7954\n",
      "Epoch 9/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3984 - accuracy: 0.8615 - loss: 0.5941 - precision: 0.8944 - recall: 0.8313 - val_NLL: 0.4804 - val_accuracy: 0.8302 - val_loss: 0.6753 - val_precision: 0.8713 - val_recall: 0.8037\n",
      "Epoch 10/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3923 - accuracy: 0.8636 - loss: 0.5880 - precision: 0.8954 - recall: 0.8344 - val_NLL: 0.4715 - val_accuracy: 0.8305 - val_loss: 0.6660 - val_precision: 0.8717 - val_recall: 0.8036\n",
      "Epoch 11/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3825 - accuracy: 0.8668 - loss: 0.5784 - precision: 0.8972 - recall: 0.8389 - val_NLL: 0.4943 - val_accuracy: 0.8283 - val_loss: 0.6892 - val_precision: 0.8670 - val_recall: 0.8006\n",
      "Epoch 12/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3742 - accuracy: 0.8698 - loss: 0.5698 - precision: 0.8987 - recall: 0.8429 - val_NLL: 0.5025 - val_accuracy: 0.8262 - val_loss: 0.6979 - val_precision: 0.8642 - val_recall: 0.8015\n",
      "Epoch 13/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3690 - accuracy: 0.8718 - loss: 0.5650 - precision: 0.8998 - recall: 0.8457 - val_NLL: 0.5013 - val_accuracy: 0.8281 - val_loss: 0.6973 - val_precision: 0.8615 - val_recall: 0.8053\n",
      "Epoch 14/30\n",
      "1215/1215 - 147s - 121ms/step - NLL: 0.3621 - accuracy: 0.8740 - loss: 0.5585 - precision: 0.9012 - recall: 0.8488 - val_NLL: 0.5276 - val_accuracy: 0.8260 - val_loss: 0.7236 - val_precision: 0.8574 - val_recall: 0.8059\n",
      "Epoch 15/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3564 - accuracy: 0.8759 - loss: 0.5528 - precision: 0.9022 - recall: 0.8515 - val_NLL: 0.5121 - val_accuracy: 0.8285 - val_loss: 0.7074 - val_precision: 0.8584 - val_recall: 0.8083\n",
      "Epoch 16/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3528 - accuracy: 0.8774 - loss: 0.5492 - precision: 0.9031 - recall: 0.8537 - val_NLL: 0.5528 - val_accuracy: 0.8215 - val_loss: 0.7490 - val_precision: 0.8513 - val_recall: 0.8013\n",
      "Epoch 17/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3495 - accuracy: 0.8786 - loss: 0.5463 - precision: 0.9039 - recall: 0.8553 - val_NLL: 0.5125 - val_accuracy: 0.8284 - val_loss: 0.7090 - val_precision: 0.8586 - val_recall: 0.8091\n",
      "Epoch 18/30\n",
      "1215/1215 - 146s - 120ms/step - NLL: 0.3426 - accuracy: 0.8806 - loss: 0.5389 - precision: 0.9050 - recall: 0.8580 - val_NLL: 0.4976 - val_accuracy: 0.8344 - val_loss: 0.6934 - val_precision: 0.8635 - val_recall: 0.8154\n",
      "Epoch 19/30\n",
      "1215/1215 - 146s - 121ms/step - NLL: 0.3385 - accuracy: 0.8821 - loss: 0.5347 - precision: 0.9059 - recall: 0.8601 - val_NLL: 0.4895 - val_accuracy: 0.8405 - val_loss: 0.6852 - val_precision: 0.8685 - val_recall: 0.8217\n",
      "Epoch 20/30\n",
      "1215/1215 - 145s - 119ms/step - NLL: 0.3338 - accuracy: 0.8835 - loss: 0.5297 - precision: 0.9065 - recall: 0.8622 - val_NLL: 0.5138 - val_accuracy: 0.8306 - val_loss: 0.7096 - val_precision: 0.8602 - val_recall: 0.8110\n",
      "\u001b[1m2429/2429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 35ms/step\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step\n",
      "NLL: 0.47\n",
      "Validation accuracy: 0.83\n",
      "Validation F1Score: 0.74\n",
      "evaluating Fold 9\n",
      "training ...\n",
      "Setting seed to 88243\n",
      "Epoch 1/30\n",
      "1194/1194 - 169s - 142ms/step - NLL: 1.1518 - accuracy: 0.6475 - loss: 1.3509 - precision: 0.8091 - recall: 0.5210 - val_NLL: 1.0478 - val_accuracy: 0.6706 - val_loss: 1.2250 - val_precision: 0.7787 - val_recall: 0.5751\n",
      "Epoch 2/30\n",
      "1194/1194 - 141s - 118ms/step - NLL: 0.7061 - accuracy: 0.7659 - loss: 0.8931 - precision: 0.8467 - recall: 0.6916 - val_NLL: 0.8621 - val_accuracy: 0.7304 - val_loss: 1.0517 - val_precision: 0.8086 - val_recall: 0.6661\n",
      "Epoch 3/30\n",
      "1194/1194 - 141s - 118ms/step - NLL: 0.5614 - accuracy: 0.8101 - loss: 0.7547 - precision: 0.8687 - recall: 0.7562 - val_NLL: 0.8299 - val_accuracy: 0.7451 - val_loss: 1.0201 - val_precision: 0.8056 - val_recall: 0.6983\n",
      "Epoch 4/30\n",
      "1194/1194 - 140s - 118ms/step - NLL: 0.5005 - accuracy: 0.8284 - loss: 0.6922 - precision: 0.8779 - recall: 0.7828 - val_NLL: 0.8187 - val_accuracy: 0.7549 - val_loss: 1.0073 - val_precision: 0.8144 - val_recall: 0.7112\n",
      "Epoch 5/30\n",
      "1194/1194 - 141s - 118ms/step - NLL: 0.4671 - accuracy: 0.8387 - loss: 0.6572 - precision: 0.8835 - recall: 0.7977 - val_NLL: 0.7593 - val_accuracy: 0.7676 - val_loss: 0.9472 - val_precision: 0.8193 - val_recall: 0.7302\n",
      "Epoch 6/30\n",
      "1194/1194 - 139s - 117ms/step - NLL: 0.4454 - accuracy: 0.8456 - loss: 0.6345 - precision: 0.8872 - recall: 0.8078 - val_NLL: 0.7883 - val_accuracy: 0.7681 - val_loss: 0.9738 - val_precision: 0.8131 - val_recall: 0.7331\n",
      "Epoch 7/30\n",
      "1194/1194 - 140s - 117ms/step - NLL: 0.4296 - accuracy: 0.8507 - loss: 0.6180 - precision: 0.8900 - recall: 0.8150 - val_NLL: 0.7836 - val_accuracy: 0.7741 - val_loss: 0.9690 - val_precision: 0.8142 - val_recall: 0.7430\n",
      "Epoch 8/30\n",
      "1194/1194 - 138s - 116ms/step - NLL: 0.4157 - accuracy: 0.8555 - loss: 0.6036 - precision: 0.8925 - recall: 0.8219 - val_NLL: 0.8043 - val_accuracy: 0.7662 - val_loss: 0.9898 - val_precision: 0.8091 - val_recall: 0.7320\n",
      "Epoch 9/30\n",
      "1194/1194 - 139s - 116ms/step - NLL: 0.4047 - accuracy: 0.8592 - loss: 0.5927 - precision: 0.8944 - recall: 0.8269 - val_NLL: 0.7817 - val_accuracy: 0.7763 - val_loss: 0.9666 - val_precision: 0.8147 - val_recall: 0.7468\n",
      "Epoch 10/30\n",
      "1194/1194 - 139s - 116ms/step - NLL: 0.3965 - accuracy: 0.8621 - loss: 0.5841 - precision: 0.8962 - recall: 0.8312 - val_NLL: 0.8087 - val_accuracy: 0.7752 - val_loss: 0.9928 - val_precision: 0.8143 - val_recall: 0.7425\n",
      "Epoch 11/30\n",
      "1194/1194 - 139s - 117ms/step - NLL: 0.3868 - accuracy: 0.8655 - loss: 0.5741 - precision: 0.8979 - recall: 0.8360 - val_NLL: 0.7849 - val_accuracy: 0.7773 - val_loss: 0.9703 - val_precision: 0.8130 - val_recall: 0.7480\n",
      "Epoch 12/30\n",
      "1194/1194 - 138s - 115ms/step - NLL: 0.3799 - accuracy: 0.8679 - loss: 0.5672 - precision: 0.8993 - recall: 0.8392 - val_NLL: 0.8194 - val_accuracy: 0.7679 - val_loss: 1.0040 - val_precision: 0.8056 - val_recall: 0.7386\n",
      "Epoch 13/30\n",
      "1194/1194 - 141s - 118ms/step - NLL: 0.3745 - accuracy: 0.8700 - loss: 0.5614 - precision: 0.9004 - recall: 0.8424 - val_NLL: 0.7934 - val_accuracy: 0.7754 - val_loss: 0.9778 - val_precision: 0.8100 - val_recall: 0.7479\n",
      "Epoch 14/30\n",
      "1194/1194 - 147s - 123ms/step - NLL: 0.3667 - accuracy: 0.8728 - loss: 0.5531 - precision: 0.9018 - recall: 0.8461 - val_NLL: 0.8288 - val_accuracy: 0.7680 - val_loss: 1.0123 - val_precision: 0.8018 - val_recall: 0.7424\n",
      "Epoch 15/30\n",
      "1194/1194 - 141s - 118ms/step - NLL: 0.3639 - accuracy: 0.8736 - loss: 0.5502 - precision: 0.9022 - recall: 0.8475 - val_NLL: 0.8176 - val_accuracy: 0.7721 - val_loss: 1.0002 - val_precision: 0.8031 - val_recall: 0.7479\n",
      "\u001b[1m2387/2387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 34ms/step\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step\n",
      "NLL: 0.76\n",
      "Validation accuracy: 0.77\n",
      "Validation F1Score: 0.61\n",
      "evaluating Fold 10\n",
      "training ...\n",
      "Setting seed to 419748\n",
      "Epoch 1/30\n",
      "1197/1197 - 149s - 124ms/step - NLL: 1.1606 - accuracy: 0.6447 - loss: 1.3554 - precision: 0.8072 - recall: 0.5185 - val_NLL: 0.7750 - val_accuracy: 0.7410 - val_loss: 0.9459 - val_precision: 0.8473 - val_recall: 0.6381\n",
      "Epoch 2/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.7304 - accuracy: 0.7600 - loss: 0.9188 - precision: 0.8447 - recall: 0.6812 - val_NLL: 0.6060 - val_accuracy: 0.7848 - val_loss: 0.7972 - val_precision: 0.8461 - val_recall: 0.7384\n",
      "Epoch 3/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.5801 - accuracy: 0.8053 - loss: 0.7764 - precision: 0.8658 - recall: 0.7485 - val_NLL: 0.5647 - val_accuracy: 0.8032 - val_loss: 0.7558 - val_precision: 0.8516 - val_recall: 0.7663\n",
      "Epoch 4/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.5188 - accuracy: 0.8241 - loss: 0.7150 - precision: 0.8759 - recall: 0.7764 - val_NLL: 0.5741 - val_accuracy: 0.8037 - val_loss: 0.7656 - val_precision: 0.8503 - val_recall: 0.7698\n",
      "Epoch 5/30\n",
      "1197/1197 - 141s - 117ms/step - NLL: 0.4817 - accuracy: 0.8350 - loss: 0.6775 - precision: 0.8815 - recall: 0.7925 - val_NLL: 0.5648 - val_accuracy: 0.8076 - val_loss: 0.7557 - val_precision: 0.8520 - val_recall: 0.7775\n",
      "Epoch 6/30\n",
      "1197/1197 - 141s - 118ms/step - NLL: 0.4597 - accuracy: 0.8418 - loss: 0.6546 - precision: 0.8850 - recall: 0.8022 - val_NLL: 0.5888 - val_accuracy: 0.8009 - val_loss: 0.7789 - val_precision: 0.8434 - val_recall: 0.7694\n",
      "Epoch 7/30\n",
      "1197/1197 - 144s - 121ms/step - NLL: 0.4400 - accuracy: 0.8477 - loss: 0.6348 - precision: 0.8880 - recall: 0.8111 - val_NLL: 0.6200 - val_accuracy: 0.7965 - val_loss: 0.8096 - val_precision: 0.8382 - val_recall: 0.7663\n",
      "Epoch 8/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.4255 - accuracy: 0.8528 - loss: 0.6195 - precision: 0.8907 - recall: 0.8182 - val_NLL: 0.5986 - val_accuracy: 0.7980 - val_loss: 0.7867 - val_precision: 0.8412 - val_recall: 0.7677\n",
      "Epoch 9/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.4153 - accuracy: 0.8561 - loss: 0.6090 - precision: 0.8925 - recall: 0.8228 - val_NLL: 0.5574 - val_accuracy: 0.8116 - val_loss: 0.7459 - val_precision: 0.8482 - val_recall: 0.7875\n",
      "Epoch 10/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.4041 - accuracy: 0.8599 - loss: 0.5977 - precision: 0.8948 - recall: 0.8280 - val_NLL: 0.5554 - val_accuracy: 0.8130 - val_loss: 0.7439 - val_precision: 0.8481 - val_recall: 0.7866\n",
      "Epoch 11/30\n",
      "1197/1197 - 141s - 118ms/step - NLL: 0.3961 - accuracy: 0.8623 - loss: 0.5890 - precision: 0.8961 - recall: 0.8314 - val_NLL: 0.6011 - val_accuracy: 0.8045 - val_loss: 0.7890 - val_precision: 0.8387 - val_recall: 0.7790\n",
      "Epoch 12/30\n",
      "1197/1197 - 141s - 118ms/step - NLL: 0.3896 - accuracy: 0.8647 - loss: 0.5822 - precision: 0.8974 - recall: 0.8347 - val_NLL: 0.5471 - val_accuracy: 0.8196 - val_loss: 0.7346 - val_precision: 0.8527 - val_recall: 0.7955\n",
      "Epoch 13/30\n",
      "1197/1197 - 139s - 116ms/step - NLL: 0.3829 - accuracy: 0.8667 - loss: 0.5756 - precision: 0.8984 - recall: 0.8375 - val_NLL: 0.6148 - val_accuracy: 0.8068 - val_loss: 0.8026 - val_precision: 0.8409 - val_recall: 0.7839\n",
      "Epoch 14/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.3769 - accuracy: 0.8689 - loss: 0.5692 - precision: 0.8998 - recall: 0.8407 - val_NLL: 0.5452 - val_accuracy: 0.8219 - val_loss: 0.7322 - val_precision: 0.8515 - val_recall: 0.8024\n",
      "Epoch 15/30\n",
      "1197/1197 - 138s - 116ms/step - NLL: 0.3720 - accuracy: 0.8708 - loss: 0.5640 - precision: 0.9007 - recall: 0.8433 - val_NLL: 0.6207 - val_accuracy: 0.8001 - val_loss: 0.8079 - val_precision: 0.8324 - val_recall: 0.7751\n",
      "Epoch 16/30\n",
      "1197/1197 - 141s - 118ms/step - NLL: 0.3684 - accuracy: 0.8720 - loss: 0.5600 - precision: 0.9016 - recall: 0.8450 - val_NLL: 0.5438 - val_accuracy: 0.8226 - val_loss: 0.7316 - val_precision: 0.8495 - val_recall: 0.8034\n",
      "Epoch 17/30\n",
      "1197/1197 - 141s - 118ms/step - NLL: 0.3652 - accuracy: 0.8732 - loss: 0.5566 - precision: 0.9022 - recall: 0.8465 - val_NLL: 0.5482 - val_accuracy: 0.8208 - val_loss: 0.7355 - val_precision: 0.8489 - val_recall: 0.8016\n",
      "Epoch 18/30\n",
      "1197/1197 - 139s - 116ms/step - NLL: 0.3616 - accuracy: 0.8745 - loss: 0.5531 - precision: 0.9029 - recall: 0.8483 - val_NLL: 0.5297 - val_accuracy: 0.8222 - val_loss: 0.7175 - val_precision: 0.8504 - val_recall: 0.8028\n",
      "Epoch 19/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.3568 - accuracy: 0.8763 - loss: 0.5478 - precision: 0.9040 - recall: 0.8508 - val_NLL: 0.5304 - val_accuracy: 0.8240 - val_loss: 0.7173 - val_precision: 0.8491 - val_recall: 0.8051\n",
      "Epoch 20/30\n",
      "1197/1197 - 139s - 116ms/step - NLL: 0.3531 - accuracy: 0.8777 - loss: 0.5439 - precision: 0.9049 - recall: 0.8528 - val_NLL: 0.5151 - val_accuracy: 0.8280 - val_loss: 0.7018 - val_precision: 0.8540 - val_recall: 0.8107\n",
      "Epoch 21/30\n",
      "1197/1197 - 138s - 115ms/step - NLL: 0.3489 - accuracy: 0.8791 - loss: 0.5396 - precision: 0.9055 - recall: 0.8547 - val_NLL: 0.5237 - val_accuracy: 0.8268 - val_loss: 0.7097 - val_precision: 0.8525 - val_recall: 0.8106\n",
      "Epoch 22/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.3449 - accuracy: 0.8804 - loss: 0.5352 - precision: 0.9064 - recall: 0.8566 - val_NLL: 0.5227 - val_accuracy: 0.8253 - val_loss: 0.7088 - val_precision: 0.8496 - val_recall: 0.8074\n",
      "Epoch 23/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.3436 - accuracy: 0.8808 - loss: 0.5341 - precision: 0.9066 - recall: 0.8575 - val_NLL: 0.5382 - val_accuracy: 0.8204 - val_loss: 0.7309 - val_precision: 0.8462 - val_recall: 0.8013\n",
      "Epoch 24/30\n",
      "1197/1197 - 138s - 116ms/step - NLL: 0.3387 - accuracy: 0.8827 - loss: 0.5285 - precision: 0.9077 - recall: 0.8598 - val_NLL: 0.5464 - val_accuracy: 0.8166 - val_loss: 0.7335 - val_precision: 0.8413 - val_recall: 0.7981\n",
      "Epoch 25/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.3385 - accuracy: 0.8830 - loss: 0.5284 - precision: 0.9076 - recall: 0.8602 - val_NLL: 0.5155 - val_accuracy: 0.8240 - val_loss: 0.7014 - val_precision: 0.8495 - val_recall: 0.8047\n",
      "Epoch 26/30\n",
      "1197/1197 - 139s - 116ms/step - NLL: 0.3348 - accuracy: 0.8842 - loss: 0.5241 - precision: 0.9085 - recall: 0.8620 - val_NLL: 0.5130 - val_accuracy: 0.8272 - val_loss: 0.6994 - val_precision: 0.8530 - val_recall: 0.8101\n",
      "Epoch 27/30\n",
      "1197/1197 - 139s - 116ms/step - NLL: 0.3333 - accuracy: 0.8849 - loss: 0.5229 - precision: 0.9088 - recall: 0.8628 - val_NLL: 0.5189 - val_accuracy: 0.8242 - val_loss: 0.7040 - val_precision: 0.8508 - val_recall: 0.8048\n",
      "Epoch 28/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.3303 - accuracy: 0.8857 - loss: 0.5198 - precision: 0.9091 - recall: 0.8641 - val_NLL: 0.5583 - val_accuracy: 0.8192 - val_loss: 0.7430 - val_precision: 0.8414 - val_recall: 0.8030\n",
      "Epoch 29/30\n",
      "1197/1197 - 141s - 118ms/step - NLL: 0.3268 - accuracy: 0.8869 - loss: 0.5159 - precision: 0.9099 - recall: 0.8658 - val_NLL: 0.5051 - val_accuracy: 0.8325 - val_loss: 0.6909 - val_precision: 0.8537 - val_recall: 0.8177\n",
      "Epoch 30/30\n",
      "1197/1197 - 140s - 117ms/step - NLL: 0.3250 - accuracy: 0.8876 - loss: 0.5145 - precision: 0.9105 - recall: 0.8667 - val_NLL: 0.5786 - val_accuracy: 0.8161 - val_loss: 0.7639 - val_precision: 0.8400 - val_recall: 0.7974\n",
      "\u001b[1m2394/2394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 35ms/step\n",
      "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step\n",
      "NLL: 0.51\n",
      "Validation accuracy: 0.83\n",
      "Validation F1Score: 0.74\n",
      "evaluating Fold 11\n",
      "training ...\n",
      "Setting seed to 786191\n",
      "Epoch 1/30\n",
      "1207/1207 - 157s - 130ms/step - NLL: 1.2279 - accuracy: 0.6375 - loss: 1.4248 - precision: 0.8189 - recall: 0.4978 - val_NLL: 0.9704 - val_accuracy: 0.6940 - val_loss: 1.1438 - val_precision: 0.7710 - val_recall: 0.6142\n",
      "Epoch 2/30\n",
      "1207/1207 - 142s - 117ms/step - NLL: 0.7524 - accuracy: 0.7548 - loss: 0.9389 - precision: 0.8487 - recall: 0.6694 - val_NLL: 0.7451 - val_accuracy: 0.7475 - val_loss: 0.9369 - val_precision: 0.8126 - val_recall: 0.6880\n",
      "Epoch 3/30\n",
      "1207/1207 - 142s - 118ms/step - NLL: 0.6068 - accuracy: 0.7976 - loss: 0.8046 - precision: 0.8665 - recall: 0.7340 - val_NLL: 0.5790 - val_accuracy: 0.7911 - val_loss: 0.7788 - val_precision: 0.8548 - val_recall: 0.7500\n",
      "Epoch 4/30\n",
      "1207/1207 - 143s - 118ms/step - NLL: 0.5308 - accuracy: 0.8207 - loss: 0.7319 - precision: 0.8769 - recall: 0.7687 - val_NLL: 0.5459 - val_accuracy: 0.8027 - val_loss: 0.7453 - val_precision: 0.8571 - val_recall: 0.7705\n",
      "Epoch 5/30\n",
      "1207/1207 - 142s - 117ms/step - NLL: 0.4858 - accuracy: 0.8338 - loss: 0.6859 - precision: 0.8826 - recall: 0.7893 - val_NLL: 0.4963 - val_accuracy: 0.8215 - val_loss: 0.6954 - val_precision: 0.8711 - val_recall: 0.7922\n",
      "Epoch 6/30\n",
      "1207/1207 - 142s - 117ms/step - NLL: 0.4586 - accuracy: 0.8426 - loss: 0.6577 - precision: 0.8865 - recall: 0.8018 - val_NLL: 0.4770 - val_accuracy: 0.8252 - val_loss: 0.6748 - val_precision: 0.8680 - val_recall: 0.7989\n",
      "Epoch 7/30\n",
      "1207/1207 - 142s - 118ms/step - NLL: 0.4406 - accuracy: 0.8482 - loss: 0.6387 - precision: 0.8893 - recall: 0.8103 - val_NLL: 0.5075 - val_accuracy: 0.8177 - val_loss: 0.7037 - val_precision: 0.8594 - val_recall: 0.7928\n",
      "Epoch 8/30\n",
      "1207/1207 - 142s - 118ms/step - NLL: 0.4258 - accuracy: 0.8533 - loss: 0.6230 - precision: 0.8916 - recall: 0.8179 - val_NLL: 0.4599 - val_accuracy: 0.8347 - val_loss: 0.6564 - val_precision: 0.8728 - val_recall: 0.8110\n",
      "Epoch 9/30\n",
      "1207/1207 - 147s - 122ms/step - NLL: 0.4134 - accuracy: 0.8575 - loss: 0.6100 - precision: 0.8936 - recall: 0.8242 - val_NLL: 0.4962 - val_accuracy: 0.8173 - val_loss: 0.6914 - val_precision: 0.8543 - val_recall: 0.7948\n",
      "Epoch 10/30\n",
      "1207/1207 - 151s - 125ms/step - NLL: 0.4040 - accuracy: 0.8608 - loss: 0.6005 - precision: 0.8951 - recall: 0.8290 - val_NLL: 0.4767 - val_accuracy: 0.8242 - val_loss: 0.6719 - val_precision: 0.8598 - val_recall: 0.8029\n",
      "Epoch 11/30\n",
      "1207/1207 - 157s - 130ms/step - NLL: 0.3955 - accuracy: 0.8637 - loss: 0.5921 - precision: 0.8965 - recall: 0.8330 - val_NLL: 0.4784 - val_accuracy: 0.8269 - val_loss: 0.6729 - val_precision: 0.8627 - val_recall: 0.8047\n",
      "Epoch 12/30\n",
      "1207/1207 - 152s - 126ms/step - NLL: 0.3877 - accuracy: 0.8666 - loss: 0.5838 - precision: 0.8982 - recall: 0.8373 - val_NLL: 0.4774 - val_accuracy: 0.8255 - val_loss: 0.6723 - val_precision: 0.8598 - val_recall: 0.8042\n",
      "Epoch 13/30\n",
      "1207/1207 - 154s - 127ms/step - NLL: 0.3808 - accuracy: 0.8688 - loss: 0.5770 - precision: 0.8995 - recall: 0.8405 - val_NLL: 0.4793 - val_accuracy: 0.8217 - val_loss: 0.6760 - val_precision: 0.8579 - val_recall: 0.7996\n",
      "Epoch 14/30\n",
      "1207/1207 - 144s - 119ms/step - NLL: 0.3751 - accuracy: 0.8710 - loss: 0.5715 - precision: 0.9004 - recall: 0.8435 - val_NLL: 0.4973 - val_accuracy: 0.8209 - val_loss: 0.6929 - val_precision: 0.8544 - val_recall: 0.7979\n",
      "Epoch 15/30\n",
      "1207/1207 - 143s - 119ms/step - NLL: 0.3683 - accuracy: 0.8734 - loss: 0.5647 - precision: 0.9018 - recall: 0.8469 - val_NLL: 0.4701 - val_accuracy: 0.8296 - val_loss: 0.6644 - val_precision: 0.8620 - val_recall: 0.8092\n",
      "Epoch 16/30\n",
      "1207/1207 - 143s - 118ms/step - NLL: 0.3631 - accuracy: 0.8753 - loss: 0.5594 - precision: 0.9028 - recall: 0.8495 - val_NLL: 0.4766 - val_accuracy: 0.8247 - val_loss: 0.6718 - val_precision: 0.8607 - val_recall: 0.8025\n",
      "Epoch 17/30\n",
      "1207/1207 - 142s - 118ms/step - NLL: 0.3581 - accuracy: 0.8771 - loss: 0.5541 - precision: 0.9038 - recall: 0.8521 - val_NLL: 0.4805 - val_accuracy: 0.8249 - val_loss: 0.6746 - val_precision: 0.8581 - val_recall: 0.8025\n",
      "Epoch 18/30\n",
      "1207/1207 - 144s - 119ms/step - NLL: 0.3533 - accuracy: 0.8787 - loss: 0.5491 - precision: 0.9047 - recall: 0.8544 - val_NLL: 0.5283 - val_accuracy: 0.8140 - val_loss: 0.7219 - val_precision: 0.8450 - val_recall: 0.7939\n",
      "\u001b[1m2413/2413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 34ms/step\n",
      "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step\n",
      "NLL: 0.46\n",
      "Validation accuracy: 0.83\n",
      "Validation F1Score: 0.74\n",
      "evaluating Fold 12\n",
      "training ...\n",
      "Setting seed to 3907426\n",
      "Epoch 1/30\n",
      "1209/1209 - 206s - 171ms/step - NLL: 1.1785 - accuracy: 0.6452 - loss: 1.3812 - precision: 0.8138 - recall: 0.5119 - val_NLL: 1.0492 - val_accuracy: 0.6646 - val_loss: 1.2213 - val_precision: 0.7824 - val_recall: 0.5717\n",
      "Epoch 2/30\n",
      "1209/1209 - 143s - 118ms/step - NLL: 0.7272 - accuracy: 0.7621 - loss: 0.9154 - precision: 0.8483 - recall: 0.6848 - val_NLL: 0.8827 - val_accuracy: 0.7196 - val_loss: 1.0788 - val_precision: 0.7956 - val_recall: 0.6645\n",
      "Epoch 3/30\n",
      "1209/1209 - 143s - 118ms/step - NLL: 0.5650 - accuracy: 0.8092 - loss: 0.7610 - precision: 0.8717 - recall: 0.7532 - val_NLL: 0.8380 - val_accuracy: 0.7320 - val_loss: 1.0320 - val_precision: 0.7934 - val_recall: 0.6841\n",
      "Epoch 4/30\n",
      "1209/1209 - 142s - 118ms/step - NLL: 0.5039 - accuracy: 0.8279 - loss: 0.6984 - precision: 0.8812 - recall: 0.7796 - val_NLL: 0.8086 - val_accuracy: 0.7435 - val_loss: 1.0013 - val_precision: 0.7950 - val_recall: 0.7026\n",
      "Epoch 5/30\n",
      "1209/1209 - 145s - 120ms/step - NLL: 0.4728 - accuracy: 0.8376 - loss: 0.6659 - precision: 0.8861 - recall: 0.7938 - val_NLL: 0.7761 - val_accuracy: 0.7580 - val_loss: 0.9673 - val_precision: 0.8043 - val_recall: 0.7193\n",
      "Epoch 6/30\n",
      "1209/1209 - 142s - 117ms/step - NLL: 0.4514 - accuracy: 0.8446 - loss: 0.6431 - precision: 0.8891 - recall: 0.8034 - val_NLL: 0.7958 - val_accuracy: 0.7577 - val_loss: 0.9861 - val_precision: 0.8021 - val_recall: 0.7206\n",
      "Epoch 7/30\n",
      "1209/1209 - 142s - 117ms/step - NLL: 0.4343 - accuracy: 0.8498 - loss: 0.6254 - precision: 0.8913 - recall: 0.8120 - val_NLL: 0.7936 - val_accuracy: 0.7640 - val_loss: 0.9833 - val_precision: 0.8086 - val_recall: 0.7337\n",
      "Epoch 8/30\n",
      "1209/1209 - 142s - 117ms/step - NLL: 0.4199 - accuracy: 0.8542 - loss: 0.6102 - precision: 0.8931 - recall: 0.8190 - val_NLL: 0.7906 - val_accuracy: 0.7651 - val_loss: 0.9801 - val_precision: 0.8051 - val_recall: 0.7329\n",
      "Epoch 9/30\n",
      "1209/1209 - 142s - 117ms/step - NLL: 0.4082 - accuracy: 0.8582 - loss: 0.5984 - precision: 0.8952 - recall: 0.8245 - val_NLL: 0.7938 - val_accuracy: 0.7622 - val_loss: 0.9828 - val_precision: 0.8026 - val_recall: 0.7327\n",
      "Epoch 10/30\n",
      "1209/1209 - 143s - 118ms/step - NLL: 0.3988 - accuracy: 0.8613 - loss: 0.5889 - precision: 0.8965 - recall: 0.8291 - val_NLL: 0.7612 - val_accuracy: 0.7637 - val_loss: 0.9503 - val_precision: 0.8038 - val_recall: 0.7336\n",
      "Epoch 11/30\n",
      "1209/1209 - 143s - 118ms/step - NLL: 0.3914 - accuracy: 0.8639 - loss: 0.5816 - precision: 0.8979 - recall: 0.8331 - val_NLL: 0.7656 - val_accuracy: 0.7671 - val_loss: 0.9545 - val_precision: 0.8042 - val_recall: 0.7372\n",
      "Epoch 12/30\n",
      "1209/1209 - 142s - 118ms/step - NLL: 0.3829 - accuracy: 0.8667 - loss: 0.5730 - precision: 0.8994 - recall: 0.8371 - val_NLL: 0.7447 - val_accuracy: 0.7691 - val_loss: 0.9336 - val_precision: 0.8048 - val_recall: 0.7427\n",
      "Epoch 13/30\n",
      "1209/1209 - 141s - 117ms/step - NLL: 0.3768 - accuracy: 0.8691 - loss: 0.5666 - precision: 0.9006 - recall: 0.8402 - val_NLL: 0.7743 - val_accuracy: 0.7670 - val_loss: 0.9629 - val_precision: 0.7981 - val_recall: 0.7425\n",
      "Epoch 14/30\n",
      "1209/1209 - 139s - 115ms/step - NLL: 0.3719 - accuracy: 0.8710 - loss: 0.5620 - precision: 0.9015 - recall: 0.8431 - val_NLL: 0.7319 - val_accuracy: 0.7743 - val_loss: 0.9210 - val_precision: 0.8099 - val_recall: 0.7474\n",
      "Epoch 15/30\n",
      "1209/1209 - 143s - 118ms/step - NLL: 0.3655 - accuracy: 0.8727 - loss: 0.5557 - precision: 0.9024 - recall: 0.8457 - val_NLL: 0.7464 - val_accuracy: 0.7701 - val_loss: 0.9355 - val_precision: 0.8028 - val_recall: 0.7459\n",
      "Epoch 16/30\n",
      "1209/1209 - 143s - 119ms/step - NLL: 0.3612 - accuracy: 0.8743 - loss: 0.5511 - precision: 0.9032 - recall: 0.8480 - val_NLL: 0.7379 - val_accuracy: 0.7744 - val_loss: 0.9272 - val_precision: 0.8085 - val_recall: 0.7494\n",
      "Epoch 17/30\n",
      "1209/1209 - 144s - 119ms/step - NLL: 0.3552 - accuracy: 0.8766 - loss: 0.5451 - precision: 0.9044 - recall: 0.8510 - val_NLL: 0.7043 - val_accuracy: 0.7861 - val_loss: 0.8926 - val_precision: 0.8177 - val_recall: 0.7626\n",
      "Epoch 18/30\n",
      "1209/1209 - 140s - 115ms/step - NLL: 0.3528 - accuracy: 0.8774 - loss: 0.5424 - precision: 0.9046 - recall: 0.8524 - val_NLL: 0.7829 - val_accuracy: 0.7677 - val_loss: 0.9717 - val_precision: 0.8046 - val_recall: 0.7431\n",
      "Epoch 19/30\n",
      "1209/1209 - 140s - 116ms/step - NLL: 0.3491 - accuracy: 0.8788 - loss: 0.5389 - precision: 0.9055 - recall: 0.8544 - val_NLL: 0.7278 - val_accuracy: 0.7772 - val_loss: 0.9162 - val_precision: 0.8084 - val_recall: 0.7575\n",
      "Epoch 20/30\n",
      "1209/1209 - 141s - 116ms/step - NLL: 0.3456 - accuracy: 0.8801 - loss: 0.5358 - precision: 0.9064 - recall: 0.8560 - val_NLL: 0.7872 - val_accuracy: 0.7750 - val_loss: 0.9766 - val_precision: 0.8049 - val_recall: 0.7559\n",
      "Epoch 21/30\n",
      "1209/1209 - 143s - 118ms/step - NLL: 0.3406 - accuracy: 0.8818 - loss: 0.5309 - precision: 0.9073 - recall: 0.8584 - val_NLL: 0.7842 - val_accuracy: 0.7749 - val_loss: 0.9741 - val_precision: 0.8023 - val_recall: 0.7556\n",
      "Epoch 22/30\n",
      "1209/1209 - 142s - 117ms/step - NLL: 0.3363 - accuracy: 0.8834 - loss: 0.5269 - precision: 0.9081 - recall: 0.8607 - val_NLL: 0.7710 - val_accuracy: 0.7788 - val_loss: 0.9607 - val_precision: 0.8068 - val_recall: 0.7612\n",
      "Epoch 23/30\n",
      "1209/1209 - 140s - 116ms/step - NLL: 0.3305 - accuracy: 0.8854 - loss: 0.5208 - precision: 0.9094 - recall: 0.8635 - val_NLL: 0.8333 - val_accuracy: 0.7654 - val_loss: 1.0224 - val_precision: 0.7938 - val_recall: 0.7450\n",
      "Epoch 24/30\n",
      "1209/1209 - 142s - 117ms/step - NLL: 0.3291 - accuracy: 0.8857 - loss: 0.5196 - precision: 0.9096 - recall: 0.8640 - val_NLL: 0.7563 - val_accuracy: 0.7746 - val_loss: 0.9457 - val_precision: 0.8042 - val_recall: 0.7548\n",
      "Epoch 25/30\n",
      "1209/1209 - 140s - 115ms/step - NLL: 0.3270 - accuracy: 0.8865 - loss: 0.5173 - precision: 0.9099 - recall: 0.8651 - val_NLL: 0.7750 - val_accuracy: 0.7738 - val_loss: 0.9642 - val_precision: 0.8053 - val_recall: 0.7529\n",
      "Epoch 26/30\n",
      "1209/1209 - 141s - 117ms/step - NLL: 0.3220 - accuracy: 0.8883 - loss: 0.5120 - precision: 0.9111 - recall: 0.8675 - val_NLL: 0.7851 - val_accuracy: 0.7763 - val_loss: 0.9741 - val_precision: 0.8038 - val_recall: 0.7587\n",
      "Epoch 27/30\n",
      "1209/1209 - 141s - 117ms/step - NLL: 0.3212 - accuracy: 0.8886 - loss: 0.5111 - precision: 0.9114 - recall: 0.8679 - val_NLL: 0.8404 - val_accuracy: 0.7629 - val_loss: 1.0290 - val_precision: 0.7884 - val_recall: 0.7449\n",
      "\u001b[1m2418/2418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 35ms/step\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step\n",
      "NLL: 0.70\n",
      "Validation accuracy: 0.79\n",
      "Validation F1Score: 0.63\n",
      "evaluating Fold 13\n",
      "training ...\n",
      "Setting seed to 9741265\n",
      "Epoch 1/30\n",
      "1224/1224 - 152s - 124ms/step - NLL: 1.1197 - accuracy: 0.6564 - loss: 1.3204 - precision: 0.8083 - recall: 0.5344 - val_NLL: 0.9948 - val_accuracy: 0.6982 - val_loss: 1.1799 - val_precision: 0.7983 - val_recall: 0.6085\n",
      "Epoch 2/30\n",
      "1224/1224 - 143s - 117ms/step - NLL: 0.6819 - accuracy: 0.7739 - loss: 0.8769 - precision: 0.8502 - recall: 0.7037 - val_NLL: 0.8008 - val_accuracy: 0.7569 - val_loss: 0.9986 - val_precision: 0.8306 - val_recall: 0.6978\n",
      "Epoch 3/30\n",
      "1224/1224 - 146s - 119ms/step - NLL: 0.5476 - accuracy: 0.8138 - loss: 0.7448 - precision: 0.8703 - recall: 0.7618 - val_NLL: 0.7888 - val_accuracy: 0.7627 - val_loss: 0.9847 - val_precision: 0.8237 - val_recall: 0.7184\n",
      "Epoch 4/30\n",
      "1224/1224 - 145s - 118ms/step - NLL: 0.4926 - accuracy: 0.8310 - loss: 0.6872 - precision: 0.8794 - recall: 0.7870 - val_NLL: 0.7762 - val_accuracy: 0.7680 - val_loss: 0.9703 - val_precision: 0.8223 - val_recall: 0.7280\n",
      "Epoch 5/30\n",
      "1224/1224 - 149s - 121ms/step - NLL: 0.4609 - accuracy: 0.8406 - loss: 0.6542 - precision: 0.8840 - recall: 0.8014 - val_NLL: 0.7975 - val_accuracy: 0.7671 - val_loss: 0.9905 - val_precision: 0.8163 - val_recall: 0.7302\n",
      "Epoch 6/30\n",
      "1224/1224 - 145s - 119ms/step - NLL: 0.4416 - accuracy: 0.8465 - loss: 0.6339 - precision: 0.8868 - recall: 0.8101 - val_NLL: 0.7975 - val_accuracy: 0.7810 - val_loss: 0.9893 - val_precision: 0.8214 - val_recall: 0.7521\n",
      "Epoch 7/30\n",
      "1224/1224 - 145s - 119ms/step - NLL: 0.4252 - accuracy: 0.8521 - loss: 0.6158 - precision: 0.8900 - recall: 0.8180 - val_NLL: 0.8811 - val_accuracy: 0.7662 - val_loss: 1.0726 - val_precision: 0.8037 - val_recall: 0.7413\n",
      "Epoch 8/30\n",
      "1224/1224 - 145s - 118ms/step - NLL: 0.4137 - accuracy: 0.8556 - loss: 0.6041 - precision: 0.8917 - recall: 0.8230 - val_NLL: 0.8731 - val_accuracy: 0.7706 - val_loss: 1.0638 - val_precision: 0.8034 - val_recall: 0.7465\n",
      "Epoch 9/30\n",
      "1224/1224 - 145s - 118ms/step - NLL: 0.4039 - accuracy: 0.8590 - loss: 0.5937 - precision: 0.8934 - recall: 0.8278 - val_NLL: 0.8518 - val_accuracy: 0.7668 - val_loss: 1.0415 - val_precision: 0.8080 - val_recall: 0.7342\n",
      "Epoch 10/30\n",
      "1224/1224 - 143s - 117ms/step - NLL: 0.3963 - accuracy: 0.8616 - loss: 0.5855 - precision: 0.8950 - recall: 0.8314 - val_NLL: 0.8529 - val_accuracy: 0.7713 - val_loss: 1.0413 - val_precision: 0.8082 - val_recall: 0.7462\n",
      "Epoch 11/30\n",
      "1224/1224 - 143s - 117ms/step - NLL: 0.3902 - accuracy: 0.8638 - loss: 0.5787 - precision: 0.8964 - recall: 0.8341 - val_NLL: 0.8891 - val_accuracy: 0.7672 - val_loss: 1.0773 - val_precision: 0.8010 - val_recall: 0.7433\n",
      "Epoch 12/30\n",
      "1224/1224 - 143s - 117ms/step - NLL: 0.3814 - accuracy: 0.8668 - loss: 0.5695 - precision: 0.8983 - recall: 0.8383 - val_NLL: 0.8559 - val_accuracy: 0.7787 - val_loss: 1.0448 - val_precision: 0.8155 - val_recall: 0.7544\n",
      "Epoch 13/30\n",
      "1224/1224 - 142s - 116ms/step - NLL: 0.3757 - accuracy: 0.8686 - loss: 0.5642 - precision: 0.8992 - recall: 0.8409 - val_NLL: 0.9255 - val_accuracy: 0.7574 - val_loss: 1.1141 - val_precision: 0.7963 - val_recall: 0.7335\n",
      "Epoch 14/30\n",
      "1224/1224 - 145s - 118ms/step - NLL: 0.3706 - accuracy: 0.8707 - loss: 0.5585 - precision: 0.9006 - recall: 0.8435 - val_NLL: 0.8521 - val_accuracy: 0.7684 - val_loss: 1.0398 - val_precision: 0.8085 - val_recall: 0.7432\n",
      "\u001b[1m2447/2447\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 34ms/step\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step\n",
      "NLL: 0.78\n",
      "Validation accuracy: 0.77\n",
      "Validation F1Score: 0.61\n",
      "evaluating Fold 14\n",
      "training ...\n",
      "Setting seed to 20465670\n",
      "Epoch 1/30\n",
      "1227/1227 - 190s - 155ms/step - NLL: 1.1263 - accuracy: 0.6564 - loss: 1.3357 - precision: 0.8088 - recall: 0.5319 - val_NLL: 0.9372 - val_accuracy: 0.7154 - val_loss: 1.1218 - val_precision: 0.8373 - val_recall: 0.6226\n",
      "Epoch 2/30\n",
      "1227/1227 - 142s - 116ms/step - NLL: 0.6871 - accuracy: 0.7727 - loss: 0.8839 - precision: 0.8500 - recall: 0.7016 - val_NLL: 0.6621 - val_accuracy: 0.7835 - val_loss: 0.8620 - val_precision: 0.8548 - val_recall: 0.7305\n",
      "Epoch 3/30\n",
      "1227/1227 - 142s - 116ms/step - NLL: 0.5510 - accuracy: 0.8131 - loss: 0.7517 - precision: 0.8706 - recall: 0.7603 - val_NLL: 0.6291 - val_accuracy: 0.7989 - val_loss: 0.8285 - val_precision: 0.8595 - val_recall: 0.7525\n",
      "Epoch 4/30\n",
      "1227/1227 - 143s - 117ms/step - NLL: 0.4936 - accuracy: 0.8305 - loss: 0.6928 - precision: 0.8794 - recall: 0.7859 - val_NLL: 0.6201 - val_accuracy: 0.8063 - val_loss: 0.8175 - val_precision: 0.8647 - val_recall: 0.7614\n",
      "Epoch 5/30\n",
      "1227/1227 - 144s - 117ms/step - NLL: 0.4623 - accuracy: 0.8404 - loss: 0.6593 - precision: 0.8843 - recall: 0.8002 - val_NLL: 0.5989 - val_accuracy: 0.8099 - val_loss: 0.7957 - val_precision: 0.8552 - val_recall: 0.7751\n",
      "Epoch 6/30\n",
      "1227/1227 - 142s - 116ms/step - NLL: 0.4420 - accuracy: 0.8469 - loss: 0.6382 - precision: 0.8873 - recall: 0.8098 - val_NLL: 0.5982 - val_accuracy: 0.8100 - val_loss: 0.7926 - val_precision: 0.8594 - val_recall: 0.7702\n",
      "Epoch 7/30\n",
      "1227/1227 - 143s - 116ms/step - NLL: 0.4276 - accuracy: 0.8514 - loss: 0.6225 - precision: 0.8897 - recall: 0.8165 - val_NLL: 0.6169 - val_accuracy: 0.8063 - val_loss: 0.8110 - val_precision: 0.8504 - val_recall: 0.7710\n",
      "Epoch 8/30\n",
      "1227/1227 - 143s - 117ms/step - NLL: 0.4154 - accuracy: 0.8552 - loss: 0.6095 - precision: 0.8917 - recall: 0.8220 - val_NLL: 0.6232 - val_accuracy: 0.8059 - val_loss: 0.8173 - val_precision: 0.8497 - val_recall: 0.7732\n",
      "Epoch 9/30\n",
      "1227/1227 - 143s - 116ms/step - NLL: 0.4061 - accuracy: 0.8584 - loss: 0.5993 - precision: 0.8933 - recall: 0.8266 - val_NLL: 0.6571 - val_accuracy: 0.7980 - val_loss: 0.8494 - val_precision: 0.8369 - val_recall: 0.7674\n",
      "Epoch 10/30\n",
      "1227/1227 - 143s - 117ms/step - NLL: 0.3977 - accuracy: 0.8615 - loss: 0.5908 - precision: 0.8948 - recall: 0.8309 - val_NLL: 0.6494 - val_accuracy: 0.8035 - val_loss: 0.8422 - val_precision: 0.8406 - val_recall: 0.7707\n",
      "Epoch 11/30\n",
      "1227/1227 - 144s - 117ms/step - NLL: 0.3889 - accuracy: 0.8643 - loss: 0.5815 - precision: 0.8960 - recall: 0.8349 - val_NLL: 0.6725 - val_accuracy: 0.8006 - val_loss: 0.8646 - val_precision: 0.8348 - val_recall: 0.7751\n",
      "Epoch 12/30\n",
      "1227/1227 - 144s - 117ms/step - NLL: 0.3822 - accuracy: 0.8669 - loss: 0.5747 - precision: 0.8975 - recall: 0.8386 - val_NLL: 0.6691 - val_accuracy: 0.7999 - val_loss: 0.8609 - val_precision: 0.8338 - val_recall: 0.7743\n",
      "Epoch 13/30\n",
      "1227/1227 - 144s - 117ms/step - NLL: 0.3739 - accuracy: 0.8695 - loss: 0.5660 - precision: 0.8990 - recall: 0.8422 - val_NLL: 0.6712 - val_accuracy: 0.7964 - val_loss: 0.8628 - val_precision: 0.8305 - val_recall: 0.7695\n",
      "Epoch 14/30\n",
      "1227/1227 - 143s - 117ms/step - NLL: 0.3703 - accuracy: 0.8709 - loss: 0.5625 - precision: 0.8999 - recall: 0.8442 - val_NLL: 0.6642 - val_accuracy: 0.7917 - val_loss: 0.8553 - val_precision: 0.8319 - val_recall: 0.7621\n",
      "Epoch 15/30\n",
      "1227/1227 - 145s - 118ms/step - NLL: 0.3655 - accuracy: 0.8727 - loss: 0.5572 - precision: 0.9005 - recall: 0.8468 - val_NLL: 0.6457 - val_accuracy: 0.8031 - val_loss: 0.8368 - val_precision: 0.8392 - val_recall: 0.7744\n",
      "Epoch 16/30\n",
      "1227/1227 - 146s - 119ms/step - NLL: 0.3586 - accuracy: 0.8750 - loss: 0.5502 - precision: 0.9020 - recall: 0.8500 - val_NLL: 0.6681 - val_accuracy: 0.7992 - val_loss: 0.8589 - val_precision: 0.8321 - val_recall: 0.7750\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 35ms/step\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step\n",
      "NLL: 0.60\n",
      "Validation accuracy: 0.81\n",
      "Validation F1Score: 0.68\n",
      "evaluating Fold 15\n",
      "training ...\n",
      "Setting seed to 80337754\n",
      "Epoch 1/30\n",
      "1227/1227 - 152s - 124ms/step - NLL: 1.1449 - accuracy: 0.6508 - loss: 1.3443 - precision: 0.8105 - recall: 0.5249 - val_NLL: 1.0858 - val_accuracy: 0.6811 - val_loss: 1.2603 - val_precision: 0.7836 - val_recall: 0.5875\n",
      "Epoch 2/30\n",
      "1227/1227 - 195s - 159ms/step - NLL: 0.7082 - accuracy: 0.7668 - loss: 0.8976 - precision: 0.8478 - recall: 0.6931 - val_NLL: 0.8034 - val_accuracy: 0.7514 - val_loss: 0.9971 - val_precision: 0.8032 - val_recall: 0.7027\n",
      "Epoch 3/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.5573 - accuracy: 0.8105 - loss: 0.7520 - precision: 0.8688 - recall: 0.7573 - val_NLL: 0.7451 - val_accuracy: 0.7657 - val_loss: 0.9365 - val_precision: 0.8078 - val_recall: 0.7276\n",
      "Epoch 4/30\n",
      "1227/1227 - 141s - 115ms/step - NLL: 0.4990 - accuracy: 0.8286 - loss: 0.6932 - precision: 0.8782 - recall: 0.7837 - val_NLL: 0.6782 - val_accuracy: 0.7805 - val_loss: 0.8694 - val_precision: 0.8176 - val_recall: 0.7516\n",
      "Epoch 5/30\n",
      "1227/1227 - 141s - 115ms/step - NLL: 0.4667 - accuracy: 0.8389 - loss: 0.6600 - precision: 0.8834 - recall: 0.7986 - val_NLL: 0.6642 - val_accuracy: 0.7827 - val_loss: 0.8544 - val_precision: 0.8174 - val_recall: 0.7547\n",
      "Epoch 6/30\n",
      "1227/1227 - 142s - 116ms/step - NLL: 0.4465 - accuracy: 0.8451 - loss: 0.6387 - precision: 0.8865 - recall: 0.8073 - val_NLL: 0.6719 - val_accuracy: 0.7810 - val_loss: 0.8609 - val_precision: 0.8150 - val_recall: 0.7554\n",
      "Epoch 7/30\n",
      "1227/1227 - 139s - 113ms/step - NLL: 0.4302 - accuracy: 0.8506 - loss: 0.6210 - precision: 0.8897 - recall: 0.8152 - val_NLL: 0.6280 - val_accuracy: 0.7873 - val_loss: 0.8159 - val_precision: 0.8179 - val_recall: 0.7638\n",
      "Epoch 8/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.4191 - accuracy: 0.8538 - loss: 0.6091 - precision: 0.8912 - recall: 0.8199 - val_NLL: 0.5977 - val_accuracy: 0.7976 - val_loss: 0.7853 - val_precision: 0.8261 - val_recall: 0.7752\n",
      "Epoch 9/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.4068 - accuracy: 0.8580 - loss: 0.5962 - precision: 0.8935 - recall: 0.8256 - val_NLL: 0.6595 - val_accuracy: 0.7866 - val_loss: 0.8468 - val_precision: 0.8149 - val_recall: 0.7650\n",
      "Epoch 10/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.3978 - accuracy: 0.8609 - loss: 0.5872 - precision: 0.8950 - recall: 0.8296 - val_NLL: 0.6424 - val_accuracy: 0.7927 - val_loss: 0.8302 - val_precision: 0.8241 - val_recall: 0.7703\n",
      "Epoch 11/30\n",
      "1227/1227 - 138s - 112ms/step - NLL: 0.3892 - accuracy: 0.8640 - loss: 0.5783 - precision: 0.8968 - recall: 0.8341 - val_NLL: 0.6600 - val_accuracy: 0.7880 - val_loss: 0.8457 - val_precision: 0.8149 - val_recall: 0.7681\n",
      "Epoch 12/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.3862 - accuracy: 0.8652 - loss: 0.5746 - precision: 0.8972 - recall: 0.8359 - val_NLL: 0.6627 - val_accuracy: 0.7927 - val_loss: 0.8480 - val_precision: 0.8167 - val_recall: 0.7746\n",
      "Epoch 13/30\n",
      "1227/1227 - 139s - 113ms/step - NLL: 0.3778 - accuracy: 0.8679 - loss: 0.5655 - precision: 0.8990 - recall: 0.8398 - val_NLL: 0.6452 - val_accuracy: 0.7915 - val_loss: 0.8302 - val_precision: 0.8168 - val_recall: 0.7725\n",
      "Epoch 14/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.3739 - accuracy: 0.8693 - loss: 0.5612 - precision: 0.8993 - recall: 0.8418 - val_NLL: 0.6002 - val_accuracy: 0.8053 - val_loss: 0.7855 - val_precision: 0.8284 - val_recall: 0.7862\n",
      "Epoch 15/30\n",
      "1227/1227 - 139s - 113ms/step - NLL: 0.3688 - accuracy: 0.8709 - loss: 0.5560 - precision: 0.9001 - recall: 0.8440 - val_NLL: 0.6609 - val_accuracy: 0.7922 - val_loss: 0.8458 - val_precision: 0.8186 - val_recall: 0.7728\n",
      "Epoch 16/30\n",
      "1227/1227 - 141s - 115ms/step - NLL: 0.3637 - accuracy: 0.8724 - loss: 0.5504 - precision: 0.9010 - recall: 0.8462 - val_NLL: 0.6515 - val_accuracy: 0.7914 - val_loss: 0.8356 - val_precision: 0.8143 - val_recall: 0.7734\n",
      "Epoch 17/30\n",
      "1227/1227 - 138s - 113ms/step - NLL: 0.3605 - accuracy: 0.8737 - loss: 0.5470 - precision: 0.9017 - recall: 0.8481 - val_NLL: 0.6408 - val_accuracy: 0.7999 - val_loss: 0.8248 - val_precision: 0.8229 - val_recall: 0.7841\n",
      "Epoch 18/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.3567 - accuracy: 0.8750 - loss: 0.5432 - precision: 0.9024 - recall: 0.8498 - val_NLL: 0.7082 - val_accuracy: 0.7872 - val_loss: 0.8917 - val_precision: 0.8107 - val_recall: 0.7680\n",
      "\u001b[1m2453/2453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 36ms/step\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step\n",
      "NLL: 0.60\n",
      "Validation accuracy: 0.80\n",
      "Validation F1Score: 0.70\n",
      "evaluating Fold 16\n",
      "training ...\n",
      "Setting seed to 123456789\n",
      "Epoch 1/30\n",
      "1231/1231 - 195s - 158ms/step - NLL: 1.1780 - accuracy: 0.6434 - loss: 1.3657 - precision: 0.8070 - recall: 0.5138 - val_NLL: 0.8727 - val_accuracy: 0.7208 - val_loss: 1.0405 - val_precision: 0.8381 - val_recall: 0.6133\n",
      "Epoch 2/30\n",
      "1231/1231 - 156s - 127ms/step - NLL: 0.7364 - accuracy: 0.7577 - loss: 0.9208 - precision: 0.8433 - recall: 0.6792 - val_NLL: 0.6781 - val_accuracy: 0.7730 - val_loss: 0.8699 - val_precision: 0.8416 - val_recall: 0.7149\n",
      "Epoch 3/30\n",
      "1231/1231 - 148s - 120ms/step - NLL: 0.5782 - accuracy: 0.8042 - loss: 0.7728 - precision: 0.8661 - recall: 0.7472 - val_NLL: 0.6902 - val_accuracy: 0.7729 - val_loss: 0.8841 - val_precision: 0.8207 - val_recall: 0.7361\n",
      "Epoch 4/30\n",
      "1231/1231 - 146s - 119ms/step - NLL: 0.5136 - accuracy: 0.8237 - loss: 0.7074 - precision: 0.8766 - recall: 0.7754 - val_NLL: 0.6359 - val_accuracy: 0.7883 - val_loss: 0.8291 - val_precision: 0.8266 - val_recall: 0.7589\n",
      "Epoch 5/30\n",
      "1231/1231 - 147s - 119ms/step - NLL: 0.4804 - accuracy: 0.8339 - loss: 0.6725 - precision: 0.8819 - recall: 0.7901 - val_NLL: 0.6821 - val_accuracy: 0.7790 - val_loss: 0.8731 - val_precision: 0.8181 - val_recall: 0.7462\n",
      "Epoch 6/30\n",
      "1231/1231 - 145s - 118ms/step - NLL: 0.4574 - accuracy: 0.8410 - loss: 0.6483 - precision: 0.8855 - recall: 0.8004 - val_NLL: 0.6143 - val_accuracy: 0.8044 - val_loss: 0.8041 - val_precision: 0.8354 - val_recall: 0.7766\n",
      "Epoch 7/30\n",
      "1231/1231 - 146s - 119ms/step - NLL: 0.4411 - accuracy: 0.8463 - loss: 0.6309 - precision: 0.8882 - recall: 0.8083 - val_NLL: 0.5793 - val_accuracy: 0.8119 - val_loss: 0.7683 - val_precision: 0.8405 - val_recall: 0.7851\n",
      "Epoch 8/30\n",
      "1231/1231 - 146s - 119ms/step - NLL: 0.4285 - accuracy: 0.8505 - loss: 0.6173 - precision: 0.8901 - recall: 0.8145 - val_NLL: 0.5948 - val_accuracy: 0.8076 - val_loss: 0.7827 - val_precision: 0.8368 - val_recall: 0.7831\n",
      "Epoch 9/30\n",
      "1231/1231 - 143s - 116ms/step - NLL: 0.4174 - accuracy: 0.8539 - loss: 0.6054 - precision: 0.8919 - recall: 0.8195 - val_NLL: 0.5727 - val_accuracy: 0.8140 - val_loss: 0.7606 - val_precision: 0.8414 - val_recall: 0.7878\n",
      "Epoch 10/30\n",
      "1231/1231 - 142s - 115ms/step - NLL: 0.4090 - accuracy: 0.8568 - loss: 0.5968 - precision: 0.8936 - recall: 0.8233 - val_NLL: 0.5611 - val_accuracy: 0.8163 - val_loss: 0.7478 - val_precision: 0.8416 - val_recall: 0.7941\n",
      "Epoch 11/30\n",
      "1231/1231 - 145s - 118ms/step - NLL: 0.4004 - accuracy: 0.8596 - loss: 0.5874 - precision: 0.8951 - recall: 0.8273 - val_NLL: 0.5715 - val_accuracy: 0.8154 - val_loss: 0.7579 - val_precision: 0.8447 - val_recall: 0.7878\n",
      "Epoch 12/30\n",
      "1231/1231 - 143s - 116ms/step - NLL: 0.3926 - accuracy: 0.8620 - loss: 0.5793 - precision: 0.8965 - recall: 0.8308 - val_NLL: 0.5559 - val_accuracy: 0.8142 - val_loss: 0.7427 - val_precision: 0.8431 - val_recall: 0.7888\n",
      "Epoch 13/30\n",
      "1231/1231 - 144s - 117ms/step - NLL: 0.3910 - accuracy: 0.8626 - loss: 0.5777 - precision: 0.8969 - recall: 0.8318 - val_NLL: 0.4955 - val_accuracy: 0.8370 - val_loss: 0.6820 - val_precision: 0.8609 - val_recall: 0.8148\n",
      "Epoch 14/30\n",
      "1231/1231 - 146s - 119ms/step - NLL: 0.3819 - accuracy: 0.8658 - loss: 0.5682 - precision: 0.8987 - recall: 0.8361 - val_NLL: 0.6137 - val_accuracy: 0.8042 - val_loss: 0.7995 - val_precision: 0.8280 - val_recall: 0.7818\n",
      "Epoch 15/30\n",
      "1231/1231 - 144s - 117ms/step - NLL: 0.3767 - accuracy: 0.8679 - loss: 0.5628 - precision: 0.8997 - recall: 0.8386 - val_NLL: 0.5455 - val_accuracy: 0.8265 - val_loss: 0.7308 - val_precision: 0.8488 - val_recall: 0.8057\n",
      "Epoch 16/30\n",
      "1231/1231 - 143s - 117ms/step - NLL: 0.3730 - accuracy: 0.8689 - loss: 0.5588 - precision: 0.9003 - recall: 0.8406 - val_NLL: 0.5599 - val_accuracy: 0.8204 - val_loss: 0.7450 - val_precision: 0.8452 - val_recall: 0.7991\n",
      "Epoch 17/30\n",
      "1231/1231 - 144s - 117ms/step - NLL: 0.3684 - accuracy: 0.8703 - loss: 0.5542 - precision: 0.9011 - recall: 0.8424 - val_NLL: 0.5771 - val_accuracy: 0.8160 - val_loss: 0.7646 - val_precision: 0.8396 - val_recall: 0.7958\n",
      "Epoch 18/30\n",
      "1231/1231 - 144s - 117ms/step - NLL: 0.3656 - accuracy: 0.8716 - loss: 0.5513 - precision: 0.9016 - recall: 0.8442 - val_NLL: 0.5551 - val_accuracy: 0.8187 - val_loss: 0.7403 - val_precision: 0.8464 - val_recall: 0.7966\n",
      "Epoch 19/30\n",
      "1231/1231 - 143s - 116ms/step - NLL: 0.3626 - accuracy: 0.8727 - loss: 0.5483 - precision: 0.9023 - recall: 0.8458 - val_NLL: 0.5327 - val_accuracy: 0.8285 - val_loss: 0.7181 - val_precision: 0.8517 - val_recall: 0.8094\n",
      "Epoch 20/30\n",
      "1231/1231 - 143s - 116ms/step - NLL: 0.3582 - accuracy: 0.8743 - loss: 0.5441 - precision: 0.9031 - recall: 0.8484 - val_NLL: 0.5467 - val_accuracy: 0.8262 - val_loss: 0.7323 - val_precision: 0.8490 - val_recall: 0.8079\n",
      "Epoch 21/30\n",
      "1231/1231 - 142s - 116ms/step - NLL: 0.3547 - accuracy: 0.8757 - loss: 0.5403 - precision: 0.9037 - recall: 0.8501 - val_NLL: 0.5791 - val_accuracy: 0.8179 - val_loss: 0.7670 - val_precision: 0.8404 - val_recall: 0.8000\n",
      "Epoch 22/30\n",
      "1231/1231 - 144s - 117ms/step - NLL: 0.3492 - accuracy: 0.8777 - loss: 0.5348 - precision: 0.9050 - recall: 0.8529 - val_NLL: 0.5767 - val_accuracy: 0.8192 - val_loss: 0.7619 - val_precision: 0.8414 - val_recall: 0.8019\n",
      "Epoch 23/30\n",
      "1231/1231 - 203s - 165ms/step - NLL: 0.3482 - accuracy: 0.8779 - loss: 0.5343 - precision: 0.9049 - recall: 0.8535 - val_NLL: 0.5563 - val_accuracy: 0.8226 - val_loss: 0.7434 - val_precision: 0.8453 - val_recall: 0.8055\n",
      "\u001b[1m2461/2461\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 38ms/step\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step\n",
      "NLL: 0.50\n",
      "Validation accuracy: 0.84\n",
      "Validation F1Score: 0.76\n",
      "evaluating Fold 17\n",
      "training ...\n",
      "Setting seed to 966726221\n",
      "Epoch 1/30\n",
      "1227/1227 - 150s - 123ms/step - NLL: 1.1352 - accuracy: 0.6556 - loss: 1.3299 - precision: 0.8099 - recall: 0.5316 - val_NLL: 1.1577 - val_accuracy: 0.6539 - val_loss: 1.3284 - val_precision: 0.7982 - val_recall: 0.5457\n",
      "Epoch 2/30\n",
      "1227/1227 - 144s - 117ms/step - NLL: 0.7221 - accuracy: 0.7624 - loss: 0.9055 - precision: 0.8437 - recall: 0.6871 - val_NLL: 0.9035 - val_accuracy: 0.7381 - val_loss: 1.0905 - val_precision: 0.8134 - val_recall: 0.6819\n",
      "Epoch 3/30\n",
      "1227/1227 - 143s - 116ms/step - NLL: 0.5730 - accuracy: 0.8059 - loss: 0.7648 - precision: 0.8651 - recall: 0.7510 - val_NLL: 0.8615 - val_accuracy: 0.7578 - val_loss: 1.0491 - val_precision: 0.8168 - val_recall: 0.7128\n",
      "Epoch 4/30\n",
      "1227/1227 - 144s - 117ms/step - NLL: 0.5065 - accuracy: 0.8261 - loss: 0.6982 - precision: 0.8752 - recall: 0.7807 - val_NLL: 0.8380 - val_accuracy: 0.7538 - val_loss: 1.0244 - val_precision: 0.8158 - val_recall: 0.7156\n",
      "Epoch 5/30\n",
      "1227/1227 - 164s - 133ms/step - NLL: 0.4715 - accuracy: 0.8367 - loss: 0.6614 - precision: 0.8808 - recall: 0.7963 - val_NLL: 0.8477 - val_accuracy: 0.7515 - val_loss: 1.0332 - val_precision: 0.8121 - val_recall: 0.7164\n",
      "Epoch 6/30\n",
      "1227/1227 - 175s - 143ms/step - NLL: 0.4496 - accuracy: 0.8439 - loss: 0.6386 - precision: 0.8849 - recall: 0.8064 - val_NLL: 0.8410 - val_accuracy: 0.7577 - val_loss: 1.0256 - val_precision: 0.8094 - val_recall: 0.7265\n",
      "Epoch 7/30\n",
      "1227/1227 - 161s - 131ms/step - NLL: 0.4352 - accuracy: 0.8489 - loss: 0.6232 - precision: 0.8875 - recall: 0.8133 - val_NLL: 0.8450 - val_accuracy: 0.7550 - val_loss: 1.0290 - val_precision: 0.8082 - val_recall: 0.7225\n",
      "Epoch 8/30\n",
      "1227/1227 - 143s - 116ms/step - NLL: 0.4237 - accuracy: 0.8525 - loss: 0.6111 - precision: 0.8895 - recall: 0.8186 - val_NLL: 0.8553 - val_accuracy: 0.7579 - val_loss: 1.0383 - val_precision: 0.8050 - val_recall: 0.7295\n",
      "Epoch 9/30\n",
      "1227/1227 - 148s - 121ms/step - NLL: 0.4133 - accuracy: 0.8559 - loss: 0.5996 - precision: 0.8914 - recall: 0.8233 - val_NLL: 0.8422 - val_accuracy: 0.7619 - val_loss: 1.0239 - val_precision: 0.8092 - val_recall: 0.7314\n",
      "Epoch 10/30\n",
      "1227/1227 - 143s - 116ms/step - NLL: 0.4032 - accuracy: 0.8592 - loss: 0.5887 - precision: 0.8933 - recall: 0.8278 - val_NLL: 0.8375 - val_accuracy: 0.7617 - val_loss: 1.0184 - val_precision: 0.8063 - val_recall: 0.7322\n",
      "Epoch 11/30\n",
      "1227/1227 - 142s - 116ms/step - NLL: 0.3979 - accuracy: 0.8612 - loss: 0.5836 - precision: 0.8945 - recall: 0.8306 - val_NLL: 0.8486 - val_accuracy: 0.7594 - val_loss: 1.0297 - val_precision: 0.8069 - val_recall: 0.7308\n",
      "Epoch 12/30\n",
      "1227/1227 - 142s - 115ms/step - NLL: 0.3910 - accuracy: 0.8635 - loss: 0.5760 - precision: 0.8959 - recall: 0.8337 - val_NLL: 0.8546 - val_accuracy: 0.7626 - val_loss: 1.0353 - val_precision: 0.8063 - val_recall: 0.7332\n",
      "Epoch 13/30\n",
      "1227/1227 - 142s - 116ms/step - NLL: 0.3837 - accuracy: 0.8657 - loss: 0.5683 - precision: 0.8970 - recall: 0.8370 - val_NLL: 0.8548 - val_accuracy: 0.7605 - val_loss: 1.0354 - val_precision: 0.8074 - val_recall: 0.7323\n",
      "Epoch 14/30\n",
      "1227/1227 - 143s - 116ms/step - NLL: 0.3781 - accuracy: 0.8680 - loss: 0.5618 - precision: 0.8986 - recall: 0.8399 - val_NLL: 0.8624 - val_accuracy: 0.7623 - val_loss: 1.0416 - val_precision: 0.8073 - val_recall: 0.7346\n",
      "Epoch 15/30\n",
      "1227/1227 - 141s - 115ms/step - NLL: 0.3737 - accuracy: 0.8693 - loss: 0.5572 - precision: 0.8990 - recall: 0.8417 - val_NLL: 0.8753 - val_accuracy: 0.7622 - val_loss: 1.0552 - val_precision: 0.7991 - val_recall: 0.7398\n",
      "Epoch 16/30\n",
      "1227/1227 - 143s - 116ms/step - NLL: 0.3695 - accuracy: 0.8711 - loss: 0.5523 - precision: 0.9001 - recall: 0.8441 - val_NLL: 0.8735 - val_accuracy: 0.7622 - val_loss: 1.0523 - val_precision: 0.8009 - val_recall: 0.7394\n",
      "Epoch 17/30\n",
      "1227/1227 - 140s - 114ms/step - NLL: 0.3650 - accuracy: 0.8723 - loss: 0.5479 - precision: 0.9009 - recall: 0.8463 - val_NLL: 0.8531 - val_accuracy: 0.7633 - val_loss: 1.0322 - val_precision: 0.8091 - val_recall: 0.7359\n",
      "Epoch 18/30\n",
      "1227/1227 - 142s - 116ms/step - NLL: 0.3603 - accuracy: 0.8742 - loss: 0.5429 - precision: 0.9019 - recall: 0.8488 - val_NLL: 0.8414 - val_accuracy: 0.7664 - val_loss: 1.0204 - val_precision: 0.8076 - val_recall: 0.7413\n",
      "Epoch 19/30\n",
      "1227/1227 - 145s - 119ms/step - NLL: 0.3563 - accuracy: 0.8755 - loss: 0.5387 - precision: 0.9028 - recall: 0.8507 - val_NLL: 0.8814 - val_accuracy: 0.7573 - val_loss: 1.0603 - val_precision: 0.7994 - val_recall: 0.7331\n",
      "Epoch 20/30\n",
      "1227/1227 - 139s - 113ms/step - NLL: 0.3517 - accuracy: 0.8772 - loss: 0.5341 - precision: 0.9036 - recall: 0.8528 - val_NLL: 0.8626 - val_accuracy: 0.7657 - val_loss: 1.0408 - val_precision: 0.8072 - val_recall: 0.7427\n",
      "\u001b[1m2453/2453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 38ms/step\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step\n",
      "NLL: 0.84\n",
      "Validation accuracy: 0.76\n",
      "Validation F1Score: 0.61\n",
      "evaluating Fold 18\n",
      "training ...\n",
      "Setting seed to 1625494306\n",
      "Epoch 1/30\n",
      "1236/1236 - 206s - 166ms/step - NLL: 1.1320 - accuracy: 0.6526 - loss: 1.3321 - precision: 0.8102 - recall: 0.5256 - val_NLL: 0.8799 - val_accuracy: 0.7011 - val_loss: 1.0587 - val_precision: 0.8019 - val_recall: 0.6115\n",
      "Epoch 2/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.7298 - accuracy: 0.7601 - loss: 0.9178 - precision: 0.8448 - recall: 0.6823 - val_NLL: 0.7575 - val_accuracy: 0.7509 - val_loss: 0.9509 - val_precision: 0.8150 - val_recall: 0.7030\n",
      "Epoch 3/30\n",
      "1236/1236 - 138s - 111ms/step - NLL: 0.5695 - accuracy: 0.8074 - loss: 0.7673 - precision: 0.8675 - recall: 0.7527 - val_NLL: 0.6987 - val_accuracy: 0.7676 - val_loss: 0.8930 - val_precision: 0.8177 - val_recall: 0.7330\n",
      "Epoch 4/30\n",
      "1236/1236 - 137s - 111ms/step - NLL: 0.5077 - accuracy: 0.8262 - loss: 0.7033 - precision: 0.8771 - recall: 0.7797 - val_NLL: 0.6353 - val_accuracy: 0.7858 - val_loss: 0.8269 - val_precision: 0.8287 - val_recall: 0.7579\n",
      "Epoch 5/30\n",
      "1236/1236 - 137s - 110ms/step - NLL: 0.4739 - accuracy: 0.8364 - loss: 0.6671 - precision: 0.8824 - recall: 0.7946 - val_NLL: 0.7073 - val_accuracy: 0.7675 - val_loss: 0.8982 - val_precision: 0.8092 - val_recall: 0.7430\n",
      "Epoch 6/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.4506 - accuracy: 0.8437 - loss: 0.6426 - precision: 0.8859 - recall: 0.8054 - val_NLL: 0.6839 - val_accuracy: 0.7709 - val_loss: 0.8744 - val_precision: 0.8094 - val_recall: 0.7445\n",
      "Epoch 7/30\n",
      "1236/1236 - 137s - 111ms/step - NLL: 0.4345 - accuracy: 0.8489 - loss: 0.6255 - precision: 0.8885 - recall: 0.8131 - val_NLL: 0.6407 - val_accuracy: 0.7821 - val_loss: 0.8297 - val_precision: 0.8192 - val_recall: 0.7570\n",
      "Epoch 8/30\n",
      "1236/1236 - 135s - 109ms/step - NLL: 0.4221 - accuracy: 0.8530 - loss: 0.6124 - precision: 0.8908 - recall: 0.8188 - val_NLL: 0.6622 - val_accuracy: 0.7811 - val_loss: 0.8514 - val_precision: 0.8176 - val_recall: 0.7582\n",
      "Epoch 9/30\n",
      "1236/1236 - 140s - 113ms/step - NLL: 0.4117 - accuracy: 0.8562 - loss: 0.6015 - precision: 0.8924 - recall: 0.8234 - val_NLL: 0.6663 - val_accuracy: 0.7826 - val_loss: 0.8531 - val_precision: 0.8143 - val_recall: 0.7621\n",
      "Epoch 10/30\n",
      "1236/1236 - 139s - 113ms/step - NLL: 0.4039 - accuracy: 0.8592 - loss: 0.5929 - precision: 0.8939 - recall: 0.8275 - val_NLL: 0.6942 - val_accuracy: 0.7738 - val_loss: 0.8820 - val_precision: 0.8095 - val_recall: 0.7513\n",
      "Epoch 11/30\n",
      "1236/1236 - 143s - 116ms/step - NLL: 0.3955 - accuracy: 0.8619 - loss: 0.5840 - precision: 0.8953 - recall: 0.8316 - val_NLL: 0.6606 - val_accuracy: 0.7820 - val_loss: 0.8483 - val_precision: 0.8155 - val_recall: 0.7590\n",
      "Epoch 12/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.3894 - accuracy: 0.8639 - loss: 0.5777 - precision: 0.8965 - recall: 0.8343 - val_NLL: 0.6880 - val_accuracy: 0.7837 - val_loss: 0.8750 - val_precision: 0.8163 - val_recall: 0.7628\n",
      "Epoch 13/30\n",
      "1236/1236 - 137s - 111ms/step - NLL: 0.3828 - accuracy: 0.8663 - loss: 0.5708 - precision: 0.8977 - recall: 0.8375 - val_NLL: 0.6699 - val_accuracy: 0.7836 - val_loss: 0.8576 - val_precision: 0.8139 - val_recall: 0.7611\n",
      "Epoch 14/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.3770 - accuracy: 0.8684 - loss: 0.5648 - precision: 0.8988 - recall: 0.8406 - val_NLL: 0.6024 - val_accuracy: 0.8009 - val_loss: 0.7922 - val_precision: 0.8276 - val_recall: 0.7777\n",
      "Epoch 15/30\n",
      "1236/1236 - 139s - 112ms/step - NLL: 0.3733 - accuracy: 0.8695 - loss: 0.5610 - precision: 0.8995 - recall: 0.8423 - val_NLL: 0.6052 - val_accuracy: 0.8044 - val_loss: 0.7907 - val_precision: 0.8321 - val_recall: 0.7814\n",
      "Epoch 16/30\n",
      "1236/1236 - 139s - 112ms/step - NLL: 0.3682 - accuracy: 0.8710 - loss: 0.5557 - precision: 0.9003 - recall: 0.8444 - val_NLL: 0.5675 - val_accuracy: 0.8094 - val_loss: 0.7521 - val_precision: 0.8340 - val_recall: 0.7873\n",
      "Epoch 17/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.3631 - accuracy: 0.8730 - loss: 0.5501 - precision: 0.9014 - recall: 0.8470 - val_NLL: 0.6155 - val_accuracy: 0.8065 - val_loss: 0.8001 - val_precision: 0.8300 - val_recall: 0.7876\n",
      "Epoch 18/30\n",
      "1236/1236 - 137s - 111ms/step - NLL: 0.3586 - accuracy: 0.8747 - loss: 0.5450 - precision: 0.9024 - recall: 0.8491 - val_NLL: 0.6680 - val_accuracy: 0.7905 - val_loss: 0.8516 - val_precision: 0.8180 - val_recall: 0.7715\n",
      "Epoch 19/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.3556 - accuracy: 0.8759 - loss: 0.5423 - precision: 0.9029 - recall: 0.8509 - val_NLL: 0.6217 - val_accuracy: 0.8038 - val_loss: 0.8057 - val_precision: 0.8281 - val_recall: 0.7853\n",
      "Epoch 20/30\n",
      "1236/1236 - 137s - 111ms/step - NLL: 0.3515 - accuracy: 0.8772 - loss: 0.5377 - precision: 0.9039 - recall: 0.8527 - val_NLL: 0.7138 - val_accuracy: 0.7914 - val_loss: 0.8974 - val_precision: 0.8168 - val_recall: 0.7755\n",
      "Epoch 21/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.3480 - accuracy: 0.8783 - loss: 0.5337 - precision: 0.9046 - recall: 0.8542 - val_NLL: 0.6594 - val_accuracy: 0.8000 - val_loss: 0.8431 - val_precision: 0.8258 - val_recall: 0.7841\n",
      "Epoch 22/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.3445 - accuracy: 0.8798 - loss: 0.5302 - precision: 0.9054 - recall: 0.8561 - val_NLL: 0.6304 - val_accuracy: 0.8115 - val_loss: 0.8135 - val_precision: 0.8337 - val_recall: 0.7941\n",
      "Epoch 23/30\n",
      "1236/1236 - 139s - 112ms/step - NLL: 0.3410 - accuracy: 0.8811 - loss: 0.5267 - precision: 0.9059 - recall: 0.8580 - val_NLL: 0.6856 - val_accuracy: 0.7988 - val_loss: 0.8689 - val_precision: 0.8222 - val_recall: 0.7821\n",
      "Epoch 24/30\n",
      "1236/1236 - 138s - 112ms/step - NLL: 0.3377 - accuracy: 0.8819 - loss: 0.5235 - precision: 0.9065 - recall: 0.8593 - val_NLL: 0.6937 - val_accuracy: 0.7973 - val_loss: 0.8771 - val_precision: 0.8246 - val_recall: 0.7785\n",
      "Epoch 25/30\n",
      "1236/1236 - 135s - 110ms/step - NLL: 0.3338 - accuracy: 0.8833 - loss: 0.5193 - precision: 0.9074 - recall: 0.8613 - val_NLL: 0.6608 - val_accuracy: 0.8055 - val_loss: 0.8431 - val_precision: 0.8278 - val_recall: 0.7892\n",
      "Epoch 26/30\n",
      "1236/1236 - 136s - 110ms/step - NLL: 0.3334 - accuracy: 0.8837 - loss: 0.5188 - precision: 0.9075 - recall: 0.8618 - val_NLL: 0.6937 - val_accuracy: 0.8021 - val_loss: 0.8777 - val_precision: 0.8254 - val_recall: 0.7862\n",
      "\u001b[1m2471/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 37ms/step\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step\n",
      "NLL: 0.57\n",
      "Validation accuracy: 0.81\n",
      "Validation F1Score: 0.70\n",
      "evaluating Fold 19\n",
      "training ...\n",
      "Setting seed to 3063573539\n",
      "Epoch 1/30\n",
      "1188/1188 - 161s - 136ms/step - NLL: 1.1536 - accuracy: 0.6476 - loss: 1.3515 - precision: 0.8075 - recall: 0.5219 - val_NLL: 0.8886 - val_accuracy: 0.7025 - val_loss: 1.0616 - val_precision: 0.7886 - val_recall: 0.5920\n",
      "Epoch 2/30\n",
      "1188/1188 - 140s - 118ms/step - NLL: 0.7363 - accuracy: 0.7576 - loss: 0.9192 - precision: 0.8431 - recall: 0.6791 - val_NLL: 0.7723 - val_accuracy: 0.7396 - val_loss: 0.9645 - val_precision: 0.8096 - val_recall: 0.6598\n",
      "Epoch 3/30\n",
      "1188/1188 - 142s - 119ms/step - NLL: 0.5879 - accuracy: 0.8011 - loss: 0.7806 - precision: 0.8645 - recall: 0.7434 - val_NLL: 0.7058 - val_accuracy: 0.7607 - val_loss: 0.8999 - val_precision: 0.8153 - val_recall: 0.7115\n",
      "Epoch 4/30\n",
      "1188/1188 - 133s - 112ms/step - NLL: 0.5188 - accuracy: 0.8224 - loss: 0.7129 - precision: 0.8755 - recall: 0.7742 - val_NLL: 0.6756 - val_accuracy: 0.7695 - val_loss: 0.8697 - val_precision: 0.8223 - val_recall: 0.7235\n",
      "Epoch 5/30\n",
      "1188/1188 - 134s - 113ms/step - NLL: 0.4798 - accuracy: 0.8341 - loss: 0.6732 - precision: 0.8816 - recall: 0.7915 - val_NLL: 0.6670 - val_accuracy: 0.7746 - val_loss: 0.8619 - val_precision: 0.8255 - val_recall: 0.7341\n",
      "Epoch 6/30\n",
      "1188/1188 - 132s - 111ms/step - NLL: 0.4554 - accuracy: 0.8420 - loss: 0.6485 - precision: 0.8858 - recall: 0.8024 - val_NLL: 0.6015 - val_accuracy: 0.7869 - val_loss: 0.7955 - val_precision: 0.8360 - val_recall: 0.7420\n",
      "Epoch 7/30\n",
      "1188/1188 - 135s - 114ms/step - NLL: 0.4376 - accuracy: 0.8476 - loss: 0.6300 - precision: 0.8887 - recall: 0.8104 - val_NLL: 0.6391 - val_accuracy: 0.7799 - val_loss: 0.8339 - val_precision: 0.8298 - val_recall: 0.7318\n",
      "Epoch 8/30\n",
      "1188/1188 - 133s - 112ms/step - NLL: 0.4246 - accuracy: 0.8516 - loss: 0.6169 - precision: 0.8908 - recall: 0.8160 - val_NLL: 0.6302 - val_accuracy: 0.7812 - val_loss: 0.8276 - val_precision: 0.8268 - val_recall: 0.7428\n",
      "Epoch 9/30\n",
      "1188/1188 - 131s - 111ms/step - NLL: 0.4111 - accuracy: 0.8558 - loss: 0.6024 - precision: 0.8932 - recall: 0.8220 - val_NLL: 0.6170 - val_accuracy: 0.7871 - val_loss: 0.8111 - val_precision: 0.8277 - val_recall: 0.7502\n",
      "Epoch 10/30\n",
      "1188/1188 - 135s - 114ms/step - NLL: 0.4044 - accuracy: 0.8581 - loss: 0.5952 - precision: 0.8945 - recall: 0.8254 - val_NLL: 0.6339 - val_accuracy: 0.7817 - val_loss: 0.8276 - val_precision: 0.8240 - val_recall: 0.7461\n",
      "Epoch 11/30\n",
      "1188/1188 - 136s - 115ms/step - NLL: 0.3968 - accuracy: 0.8608 - loss: 0.5872 - precision: 0.8956 - recall: 0.8291 - val_NLL: 0.6172 - val_accuracy: 0.7874 - val_loss: 0.8093 - val_precision: 0.8229 - val_recall: 0.7577\n",
      "Epoch 12/30\n",
      "1188/1188 - 134s - 112ms/step - NLL: 0.3894 - accuracy: 0.8630 - loss: 0.5793 - precision: 0.8971 - recall: 0.8322 - val_NLL: 0.5968 - val_accuracy: 0.7902 - val_loss: 0.7874 - val_precision: 0.8340 - val_recall: 0.7514\n",
      "Epoch 13/30\n",
      "1188/1188 - 134s - 112ms/step - NLL: 0.3847 - accuracy: 0.8647 - loss: 0.5745 - precision: 0.8980 - recall: 0.8345 - val_NLL: 0.5909 - val_accuracy: 0.7960 - val_loss: 0.7812 - val_precision: 0.8323 - val_recall: 0.7652\n",
      "Epoch 14/30\n",
      "1188/1188 - 134s - 113ms/step - NLL: 0.3777 - accuracy: 0.8673 - loss: 0.5667 - precision: 0.8994 - recall: 0.8382 - val_NLL: 0.6348 - val_accuracy: 0.7919 - val_loss: 0.8235 - val_precision: 0.8286 - val_recall: 0.7575\n",
      "Epoch 15/30\n",
      "1188/1188 - 132s - 111ms/step - NLL: 0.3730 - accuracy: 0.8691 - loss: 0.5616 - precision: 0.9004 - recall: 0.8408 - val_NLL: 0.5943 - val_accuracy: 0.7958 - val_loss: 0.7882 - val_precision: 0.8287 - val_recall: 0.7677\n",
      "Epoch 16/30\n",
      "1188/1188 - 134s - 112ms/step - NLL: 0.3704 - accuracy: 0.8700 - loss: 0.5590 - precision: 0.9004 - recall: 0.8422 - val_NLL: 0.6078 - val_accuracy: 0.7979 - val_loss: 0.7978 - val_precision: 0.8273 - val_recall: 0.7711\n",
      "Epoch 17/30\n",
      "1188/1188 - 135s - 113ms/step - NLL: 0.3627 - accuracy: 0.8730 - loss: 0.5509 - precision: 0.9023 - recall: 0.8464 - val_NLL: 0.5859 - val_accuracy: 0.8063 - val_loss: 0.7757 - val_precision: 0.8349 - val_recall: 0.7817\n",
      "Epoch 18/30\n",
      "1188/1188 - 133s - 112ms/step - NLL: 0.3612 - accuracy: 0.8736 - loss: 0.5495 - precision: 0.9026 - recall: 0.8472 - val_NLL: 0.6085 - val_accuracy: 0.8019 - val_loss: 0.7982 - val_precision: 0.8311 - val_recall: 0.7752\n",
      "Epoch 19/30\n",
      "1188/1188 - 135s - 114ms/step - NLL: 0.3554 - accuracy: 0.8757 - loss: 0.5435 - precision: 0.9037 - recall: 0.8501 - val_NLL: 0.6381 - val_accuracy: 0.7928 - val_loss: 0.8267 - val_precision: 0.8228 - val_recall: 0.7679\n",
      "Epoch 20/30\n",
      "1188/1188 - 131s - 110ms/step - NLL: 0.3510 - accuracy: 0.8772 - loss: 0.5396 - precision: 0.9044 - recall: 0.8525 - val_NLL: 0.5644 - val_accuracy: 0.8143 - val_loss: 0.7563 - val_precision: 0.8407 - val_recall: 0.7925\n",
      "Epoch 21/30\n",
      "1188/1188 - 132s - 112ms/step - NLL: 0.3472 - accuracy: 0.8789 - loss: 0.5355 - precision: 0.9053 - recall: 0.8550 - val_NLL: 0.6011 - val_accuracy: 0.8001 - val_loss: 0.7924 - val_precision: 0.8280 - val_recall: 0.7748\n",
      "Epoch 22/30\n",
      "1188/1188 - 133s - 112ms/step - NLL: 0.3440 - accuracy: 0.8802 - loss: 0.5327 - precision: 0.9060 - recall: 0.8564 - val_NLL: 0.5598 - val_accuracy: 0.8098 - val_loss: 0.7508 - val_precision: 0.8385 - val_recall: 0.7852\n",
      "Epoch 23/30\n",
      "1188/1188 - 132s - 111ms/step - NLL: 0.3412 - accuracy: 0.8810 - loss: 0.5306 - precision: 0.9064 - recall: 0.8578 - val_NLL: 0.6474 - val_accuracy: 0.7917 - val_loss: 0.8417 - val_precision: 0.8205 - val_recall: 0.7670\n",
      "Epoch 24/30\n",
      "1188/1188 - 130s - 109ms/step - NLL: 0.3374 - accuracy: 0.8825 - loss: 0.5269 - precision: 0.9073 - recall: 0.8602 - val_NLL: 0.6439 - val_accuracy: 0.7939 - val_loss: 0.8362 - val_precision: 0.8191 - val_recall: 0.7743\n",
      "Epoch 25/30\n",
      "1188/1188 - 134s - 113ms/step - NLL: 0.3326 - accuracy: 0.8845 - loss: 0.5217 - precision: 0.9084 - recall: 0.8625 - val_NLL: 0.6886 - val_accuracy: 0.7887 - val_loss: 0.8803 - val_precision: 0.8135 - val_recall: 0.7687\n",
      "Epoch 26/30\n",
      "1188/1188 - 133s - 112ms/step - NLL: 0.3298 - accuracy: 0.8855 - loss: 0.5192 - precision: 0.9089 - recall: 0.8640 - val_NLL: 0.6639 - val_accuracy: 0.7912 - val_loss: 0.8558 - val_precision: 0.8168 - val_recall: 0.7698\n",
      "Epoch 27/30\n",
      "1188/1188 - 132s - 111ms/step - NLL: 0.3270 - accuracy: 0.8864 - loss: 0.5161 - precision: 0.9096 - recall: 0.8653 - val_NLL: 0.6027 - val_accuracy: 0.8020 - val_loss: 0.7924 - val_precision: 0.8308 - val_recall: 0.7798\n",
      "Epoch 28/30\n",
      "1188/1188 - 140s - 118ms/step - NLL: 0.3234 - accuracy: 0.8878 - loss: 0.5125 - precision: 0.9104 - recall: 0.8670 - val_NLL: 0.6334 - val_accuracy: 0.7996 - val_loss: 0.8272 - val_precision: 0.8239 - val_recall: 0.7790\n",
      "Epoch 29/30\n",
      "1188/1188 - 135s - 113ms/step - NLL: 0.3208 - accuracy: 0.8886 - loss: 0.5098 - precision: 0.9110 - recall: 0.8681 - val_NLL: 0.6358 - val_accuracy: 0.8029 - val_loss: 0.8274 - val_precision: 0.8262 - val_recall: 0.7843\n",
      "Epoch 30/30\n",
      "1188/1188 - 137s - 116ms/step - NLL: 0.3191 - accuracy: 0.8891 - loss: 0.5084 - precision: 0.9112 - recall: 0.8690 - val_NLL: 0.6482 - val_accuracy: 0.8004 - val_loss: 0.8388 - val_precision: 0.8252 - val_recall: 0.7799\n",
      "\u001b[1m2375/2375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 46ms/step\n",
      "\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 36ms/step\n",
      "NLL: 0.56\n",
      "Validation accuracy: 0.81\n",
      "Validation F1Score: 0.72\n",
      "evaluating Fold 20\n",
      "training ...\n",
      "Setting seed to 1898784207\n",
      "Epoch 1/30\n",
      "1218/1218 - 185s - 152ms/step - NLL: 1.1062 - accuracy: 0.6600 - loss: 1.3091 - precision: 0.8136 - recall: 0.5352 - val_NLL: 1.2246 - val_accuracy: 0.6197 - val_loss: 1.4014 - val_precision: 0.7365 - val_recall: 0.5255\n",
      "Epoch 2/30\n",
      "1218/1218 - 160s - 131ms/step - NLL: 0.7140 - accuracy: 0.7650 - loss: 0.9026 - precision: 0.8469 - recall: 0.6903 - val_NLL: 1.0312 - val_accuracy: 0.6706 - val_loss: 1.2267 - val_precision: 0.7566 - val_recall: 0.6112\n",
      "Epoch 3/30\n",
      "1218/1218 - 146s - 120ms/step - NLL: 0.5588 - accuracy: 0.8115 - loss: 0.7568 - precision: 0.8700 - recall: 0.7580 - val_NLL: 0.8712 - val_accuracy: 0.7257 - val_loss: 1.0771 - val_precision: 0.7853 - val_recall: 0.6826\n",
      "Epoch 4/30\n",
      "1218/1218 - 146s - 120ms/step - NLL: 0.4960 - accuracy: 0.8305 - loss: 0.6936 - precision: 0.8795 - recall: 0.7858 - val_NLL: 0.8636 - val_accuracy: 0.7386 - val_loss: 1.0695 - val_precision: 0.7859 - val_recall: 0.7014\n",
      "Epoch 5/30\n",
      "1218/1218 - 147s - 121ms/step - NLL: 0.4617 - accuracy: 0.8415 - loss: 0.6568 - precision: 0.8852 - recall: 0.8015 - val_NLL: 0.8541 - val_accuracy: 0.7429 - val_loss: 1.0634 - val_precision: 0.7869 - val_recall: 0.7090\n",
      "Epoch 6/30\n",
      "1218/1218 - 144s - 118ms/step - NLL: 0.4406 - accuracy: 0.8480 - loss: 0.6341 - precision: 0.8883 - recall: 0.8111 - val_NLL: 0.9098 - val_accuracy: 0.7374 - val_loss: 1.1198 - val_precision: 0.7779 - val_recall: 0.7069\n",
      "Epoch 7/30\n",
      "1218/1218 - 141s - 115ms/step - NLL: 0.4257 - accuracy: 0.8530 - loss: 0.6188 - precision: 0.8911 - recall: 0.8182 - val_NLL: 0.8947 - val_accuracy: 0.7399 - val_loss: 1.1003 - val_precision: 0.7799 - val_recall: 0.7100\n",
      "Epoch 8/30\n",
      "1218/1218 - 144s - 118ms/step - NLL: 0.4131 - accuracy: 0.8570 - loss: 0.6057 - precision: 0.8932 - recall: 0.8240 - val_NLL: 0.9163 - val_accuracy: 0.7445 - val_loss: 1.1246 - val_precision: 0.7810 - val_recall: 0.7180\n",
      "Epoch 9/30\n",
      "1218/1218 - 142s - 116ms/step - NLL: 0.4036 - accuracy: 0.8603 - loss: 0.5963 - precision: 0.8948 - recall: 0.8285 - val_NLL: 0.8922 - val_accuracy: 0.7475 - val_loss: 1.0955 - val_precision: 0.7869 - val_recall: 0.7181\n",
      "Epoch 10/30\n",
      "1218/1218 - 142s - 116ms/step - NLL: 0.3931 - accuracy: 0.8634 - loss: 0.5857 - precision: 0.8964 - recall: 0.8331 - val_NLL: 0.9002 - val_accuracy: 0.7449 - val_loss: 1.1121 - val_precision: 0.7818 - val_recall: 0.7201\n",
      "Epoch 11/30\n",
      "1218/1218 - 142s - 116ms/step - NLL: 0.3833 - accuracy: 0.8669 - loss: 0.5751 - precision: 0.8987 - recall: 0.8375 - val_NLL: 0.9080 - val_accuracy: 0.7529 - val_loss: 1.1165 - val_precision: 0.7886 - val_recall: 0.7285\n",
      "Epoch 12/30\n",
      "1218/1218 - 141s - 116ms/step - NLL: 0.3758 - accuracy: 0.8692 - loss: 0.5674 - precision: 0.8996 - recall: 0.8411 - val_NLL: 0.9103 - val_accuracy: 0.7509 - val_loss: 1.1162 - val_precision: 0.7840 - val_recall: 0.7289\n",
      "Epoch 13/30\n",
      "1218/1218 - 143s - 117ms/step - NLL: 0.3702 - accuracy: 0.8712 - loss: 0.5620 - precision: 0.9008 - recall: 0.8436 - val_NLL: 0.9172 - val_accuracy: 0.7454 - val_loss: 1.1163 - val_precision: 0.7836 - val_recall: 0.7193\n",
      "Epoch 14/30\n",
      "1218/1218 - 142s - 117ms/step - NLL: 0.3632 - accuracy: 0.8736 - loss: 0.5548 - precision: 0.9021 - recall: 0.8471 - val_NLL: 0.9734 - val_accuracy: 0.7377 - val_loss: 1.1726 - val_precision: 0.7705 - val_recall: 0.7156\n",
      "Epoch 15/30\n",
      "1218/1218 - 139s - 114ms/step - NLL: 0.3576 - accuracy: 0.8756 - loss: 0.5491 - precision: 0.9033 - recall: 0.8500 - val_NLL: 0.9277 - val_accuracy: 0.7489 - val_loss: 1.1272 - val_precision: 0.7812 - val_recall: 0.7286\n",
      "\u001b[1m2436/2436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 38ms/step\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step\n",
      "NLL: 0.85\n",
      "Validation accuracy: 0.74\n",
      "Validation F1Score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "\n",
    "results_valid_labels_list = []\n",
    "results_scores_list_valid = []\n",
    "results_scores_list_train = []\n",
    "\n",
    "for seedidx, n_fold in enumerate(folds):\n",
    "    print('evaluating Fold', n_fold)\n",
    "    seed_val = seeds[seedidx]\n",
    "\n",
    "    fold = pickle.load(open((datapath / ('Fold' + str(n_fold) + '.pkl')).resolve(), 'rb'))\n",
    "    Y_train_oh = fold['train']['Y_allframes_oh']\n",
    "    Y_valid_oh = fold['valid']['Y_allframes_oh']\n",
    "    Y_valid = fold['valid']['Y_allframes']\n",
    "    X_train = fold['train']['X']\n",
    "    X_valid = fold['valid']['X']\n",
    "    print('training ...')\n",
    "\n",
    "\n",
    "    mdl13 = ConvModel(cfg, 'Test', seed = seed_val)\n",
    "    mdl13.compile_model()\n",
    "\n",
    "    # Train the model\n",
    "    mdl13.train_model(X_train, Y_train_oh, X_valid, Y_valid_oh)\n",
    "\n",
    "    Y_prob_train = mdl13.model.predict(X_train)\n",
    "    Y_prob_valid = mdl13.model.predict(X_valid)\n",
    "    # Y_score_true = np.max(Y_prob_valid, axis=-1).flatten()\n",
    "    Y_score_true = Y_prob_valid[np.arange(Y_valid.shape[0])[:, None], np.arange(Y_valid.shape[1]), Y_valid].flatten()\n",
    "\n",
    "    Y_hat_train = np.argmax(Y_prob_train, axis=-1).flatten()\n",
    "    Y_hat_valid = np.argmax(Y_prob_valid, axis=-1).flatten()\n",
    "\n",
    "    results_train = mdl13.model.evaluate(X_train, Y_train_oh, verbose = 0, return_dict=True)\n",
    "    results_train['f1_score'] = f1_score(fold['train']['Y_allframes'].flatten(), Y_hat_train, average='macro')\n",
    "\n",
    "    results_valid = mdl13.model.evaluate(X_valid, Y_valid_oh, verbose = 0, return_dict=True)\n",
    "    results_valid['f1_score'] = f1_score(fold['valid']['Y_allframes'].flatten(), Y_hat_valid, average='macro')\n",
    "\n",
    "    results_valid['fold'] = n_fold\n",
    "\n",
    "    print(f'NLL: {results_valid['NLL']:,.2f}')\n",
    "    print(f'Validation accuracy: {results_valid['accuracy']:,.2f}')\n",
    "    print(f'Validation F1Score: {results_valid['f1_score']:,.2f}')\n",
    "\n",
    "\n",
    "    #caution from here\n",
    "    results_valid_labels_list.append(pd.DataFrame({'Fold': n_fold, 'Y': fold['valid']['Y_allframes'].flatten(), 'Y_hat': Y_hat_valid, 'Y_prob': Y_score_true}))\n",
    "    results_scores_list_train.append(pd.DataFrame(results_train, index = [n_fold]))\n",
    "    results_scores_list_valid.append(pd.DataFrame(results_valid, index = [n_fold]))\n",
    "    #results_scores_list.append({'Fold': n_fold, 'Accuracy_train': accuracy_train, 'Accuracy_valid': accuracy_valid, 'F1-Score_train': f1_score_train, 'F1-Score_valid': f1_score_valid, 'NLL_train': nll_train, 'NLL_valid': nll_valid})\n",
    "    \n",
    "results_scores = pd.concat([pd.concat(results_scores_list_train).add_suffix('_train'),pd.concat(results_scores_list_valid).add_suffix('_valid')],axis = 1)\n",
    "results_scores.rename(columns = {'fold_train': 'fold'}, inplace = True)\n",
    "results_valid_labels = pd.concat(results_valid_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_valid_labels.to_pickle(datapath/'results_valid_labels.pkl')\n",
    "results_scores.to_pickle(datapath/'results_scores.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLL_train</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>precision_train</th>\n",
       "      <th>recall_train</th>\n",
       "      <th>f1_score_train</th>\n",
       "      <th>NLL_valid</th>\n",
       "      <th>accuracy_valid</th>\n",
       "      <th>loss_valid</th>\n",
       "      <th>precision_valid</th>\n",
       "      <th>recall_valid</th>\n",
       "      <th>f1_score_valid</th>\n",
       "      <th>fold_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.385008</td>\n",
       "      <td>0.863444</td>\n",
       "      <td>0.578677</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.832620</td>\n",
       "      <td>0.793613</td>\n",
       "      <td>0.665227</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.859126</td>\n",
       "      <td>0.823790</td>\n",
       "      <td>0.750615</td>\n",
       "      <td>0.651938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.389026</td>\n",
       "      <td>0.861606</td>\n",
       "      <td>0.584013</td>\n",
       "      <td>0.894719</td>\n",
       "      <td>0.832997</td>\n",
       "      <td>0.784347</td>\n",
       "      <td>0.971316</td>\n",
       "      <td>0.724225</td>\n",
       "      <td>1.166631</td>\n",
       "      <td>0.785098</td>\n",
       "      <td>0.697033</td>\n",
       "      <td>0.453930</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.326934</td>\n",
       "      <td>0.882922</td>\n",
       "      <td>0.521456</td>\n",
       "      <td>0.912635</td>\n",
       "      <td>0.855704</td>\n",
       "      <td>0.826102</td>\n",
       "      <td>0.581181</td>\n",
       "      <td>0.812621</td>\n",
       "      <td>0.775971</td>\n",
       "      <td>0.841718</td>\n",
       "      <td>0.789725</td>\n",
       "      <td>0.723710</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.391040</td>\n",
       "      <td>0.859429</td>\n",
       "      <td>0.588602</td>\n",
       "      <td>0.890935</td>\n",
       "      <td>0.832517</td>\n",
       "      <td>0.781430</td>\n",
       "      <td>0.561893</td>\n",
       "      <td>0.812653</td>\n",
       "      <td>0.764584</td>\n",
       "      <td>0.847478</td>\n",
       "      <td>0.787075</td>\n",
       "      <td>0.646243</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.361692</td>\n",
       "      <td>0.870215</td>\n",
       "      <td>0.555491</td>\n",
       "      <td>0.904206</td>\n",
       "      <td>0.839950</td>\n",
       "      <td>0.805061</td>\n",
       "      <td>0.617805</td>\n",
       "      <td>0.807268</td>\n",
       "      <td>0.811428</td>\n",
       "      <td>0.838356</td>\n",
       "      <td>0.775672</td>\n",
       "      <td>0.661167</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.371030</td>\n",
       "      <td>0.865852</td>\n",
       "      <td>0.568357</td>\n",
       "      <td>0.897444</td>\n",
       "      <td>0.837603</td>\n",
       "      <td>0.795591</td>\n",
       "      <td>0.657011</td>\n",
       "      <td>0.799251</td>\n",
       "      <td>0.854589</td>\n",
       "      <td>0.847649</td>\n",
       "      <td>0.767156</td>\n",
       "      <td>0.600589</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.344297</td>\n",
       "      <td>0.876678</td>\n",
       "      <td>0.541302</td>\n",
       "      <td>0.904862</td>\n",
       "      <td>0.851306</td>\n",
       "      <td>0.815235</td>\n",
       "      <td>0.682715</td>\n",
       "      <td>0.767962</td>\n",
       "      <td>0.879525</td>\n",
       "      <td>0.809568</td>\n",
       "      <td>0.732279</td>\n",
       "      <td>0.647981</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.325365</td>\n",
       "      <td>0.881818</td>\n",
       "      <td>0.520870</td>\n",
       "      <td>0.907822</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.827224</td>\n",
       "      <td>0.471508</td>\n",
       "      <td>0.830524</td>\n",
       "      <td>0.665830</td>\n",
       "      <td>0.871739</td>\n",
       "      <td>0.803608</td>\n",
       "      <td>0.735090</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.865875</td>\n",
       "      <td>0.567579</td>\n",
       "      <td>0.903216</td>\n",
       "      <td>0.832124</td>\n",
       "      <td>0.799492</td>\n",
       "      <td>0.759284</td>\n",
       "      <td>0.767631</td>\n",
       "      <td>0.949069</td>\n",
       "      <td>0.819288</td>\n",
       "      <td>0.730189</td>\n",
       "      <td>0.614522</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.263761</td>\n",
       "      <td>0.905217</td>\n",
       "      <td>0.453359</td>\n",
       "      <td>0.920825</td>\n",
       "      <td>0.891567</td>\n",
       "      <td>0.867619</td>\n",
       "      <td>0.505105</td>\n",
       "      <td>0.832471</td>\n",
       "      <td>0.693001</td>\n",
       "      <td>0.853694</td>\n",
       "      <td>0.817732</td>\n",
       "      <td>0.739106</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.356232</td>\n",
       "      <td>0.871564</td>\n",
       "      <td>0.553882</td>\n",
       "      <td>0.901301</td>\n",
       "      <td>0.845008</td>\n",
       "      <td>0.804279</td>\n",
       "      <td>0.459925</td>\n",
       "      <td>0.834661</td>\n",
       "      <td>0.655743</td>\n",
       "      <td>0.872764</td>\n",
       "      <td>0.810993</td>\n",
       "      <td>0.738429</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.293419</td>\n",
       "      <td>0.894581</td>\n",
       "      <td>0.481788</td>\n",
       "      <td>0.916469</td>\n",
       "      <td>0.874749</td>\n",
       "      <td>0.848373</td>\n",
       "      <td>0.704336</td>\n",
       "      <td>0.786120</td>\n",
       "      <td>0.892559</td>\n",
       "      <td>0.817709</td>\n",
       "      <td>0.762636</td>\n",
       "      <td>0.634734</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.425205</td>\n",
       "      <td>0.848966</td>\n",
       "      <td>0.618980</td>\n",
       "      <td>0.892558</td>\n",
       "      <td>0.810589</td>\n",
       "      <td>0.777458</td>\n",
       "      <td>0.776168</td>\n",
       "      <td>0.768034</td>\n",
       "      <td>0.969031</td>\n",
       "      <td>0.822309</td>\n",
       "      <td>0.727989</td>\n",
       "      <td>0.605484</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.367929</td>\n",
       "      <td>0.867382</td>\n",
       "      <td>0.562689</td>\n",
       "      <td>0.902517</td>\n",
       "      <td>0.836347</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>0.598215</td>\n",
       "      <td>0.810003</td>\n",
       "      <td>0.792687</td>\n",
       "      <td>0.859377</td>\n",
       "      <td>0.770223</td>\n",
       "      <td>0.681377</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.344987</td>\n",
       "      <td>0.873878</td>\n",
       "      <td>0.533997</td>\n",
       "      <td>0.904242</td>\n",
       "      <td>0.847751</td>\n",
       "      <td>0.816490</td>\n",
       "      <td>0.597674</td>\n",
       "      <td>0.797584</td>\n",
       "      <td>0.786798</td>\n",
       "      <td>0.826060</td>\n",
       "      <td>0.775156</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.339143</td>\n",
       "      <td>0.875073</td>\n",
       "      <td>0.525711</td>\n",
       "      <td>0.902726</td>\n",
       "      <td>0.852164</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.495484</td>\n",
       "      <td>0.837049</td>\n",
       "      <td>0.682045</td>\n",
       "      <td>0.860906</td>\n",
       "      <td>0.814834</td>\n",
       "      <td>0.759231</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.328967</td>\n",
       "      <td>0.880742</td>\n",
       "      <td>0.513349</td>\n",
       "      <td>0.907157</td>\n",
       "      <td>0.857323</td>\n",
       "      <td>0.822991</td>\n",
       "      <td>0.837515</td>\n",
       "      <td>0.761707</td>\n",
       "      <td>1.018111</td>\n",
       "      <td>0.806302</td>\n",
       "      <td>0.732190</td>\n",
       "      <td>0.610393</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.337093</td>\n",
       "      <td>0.877426</td>\n",
       "      <td>0.523365</td>\n",
       "      <td>0.902770</td>\n",
       "      <td>0.855272</td>\n",
       "      <td>0.821124</td>\n",
       "      <td>0.567542</td>\n",
       "      <td>0.809420</td>\n",
       "      <td>0.750894</td>\n",
       "      <td>0.834026</td>\n",
       "      <td>0.787257</td>\n",
       "      <td>0.700834</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.287476</td>\n",
       "      <td>0.894672</td>\n",
       "      <td>0.476962</td>\n",
       "      <td>0.916078</td>\n",
       "      <td>0.875203</td>\n",
       "      <td>0.849344</td>\n",
       "      <td>0.559805</td>\n",
       "      <td>0.809790</td>\n",
       "      <td>0.750686</td>\n",
       "      <td>0.838485</td>\n",
       "      <td>0.785157</td>\n",
       "      <td>0.716605</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.371847</td>\n",
       "      <td>0.868863</td>\n",
       "      <td>0.566636</td>\n",
       "      <td>0.903694</td>\n",
       "      <td>0.837608</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>0.854064</td>\n",
       "      <td>0.742851</td>\n",
       "      <td>1.054869</td>\n",
       "      <td>0.786894</td>\n",
       "      <td>0.708999</td>\n",
       "      <td>0.655696</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NLL_train  accuracy_train  loss_train  precision_train  recall_train  \\\n",
       "1    0.385008        0.863444    0.578677         0.898482      0.832620   \n",
       "2    0.389026        0.861606    0.584013         0.894719      0.832997   \n",
       "3    0.326934        0.882922    0.521456         0.912635      0.855704   \n",
       "4    0.391040        0.859429    0.588602         0.890935      0.832517   \n",
       "5    0.361692        0.870215    0.555491         0.904206      0.839950   \n",
       "6    0.371030        0.865852    0.568357         0.897444      0.837603   \n",
       "7    0.344297        0.876678    0.541302         0.904862      0.851306   \n",
       "8    0.325365        0.881818    0.520870         0.907822      0.859259   \n",
       "9    0.377500        0.865875    0.567579         0.903216      0.832124   \n",
       "10   0.263761        0.905217    0.453359         0.920825      0.891567   \n",
       "11   0.356232        0.871564    0.553882         0.901301      0.845008   \n",
       "12   0.293419        0.894581    0.481788         0.916469      0.874749   \n",
       "13   0.425205        0.848966    0.618980         0.892558      0.810589   \n",
       "14   0.367929        0.867382    0.562689         0.902517      0.836347   \n",
       "15   0.344987        0.873878    0.533997         0.904242      0.847751   \n",
       "16   0.339143        0.875073    0.525711         0.902726      0.852164   \n",
       "17   0.328967        0.880742    0.513349         0.907157      0.857323   \n",
       "18   0.337093        0.877426    0.523365         0.902770      0.855272   \n",
       "19   0.287476        0.894672    0.476962         0.916078      0.875203   \n",
       "20   0.371847        0.868863    0.566636         0.903694      0.837608   \n",
       "\n",
       "    f1_score_train  NLL_valid  accuracy_valid  loss_valid  precision_valid  \\\n",
       "1         0.793613   0.665227        0.779800    0.859126         0.823790   \n",
       "2         0.784347   0.971316        0.724225    1.166631         0.785098   \n",
       "3         0.826102   0.581181        0.812621    0.775971         0.841718   \n",
       "4         0.781430   0.561893        0.812653    0.764584         0.847478   \n",
       "5         0.805061   0.617805        0.807268    0.811428         0.838356   \n",
       "6         0.795591   0.657011        0.799251    0.854589         0.847649   \n",
       "7         0.815235   0.682715        0.767962    0.879525         0.809568   \n",
       "8         0.827224   0.471508        0.830524    0.665830         0.871739   \n",
       "9         0.799492   0.759284        0.767631    0.949069         0.819288   \n",
       "10        0.867619   0.505105        0.832471    0.693001         0.853694   \n",
       "11        0.804279   0.459925        0.834661    0.655743         0.872764   \n",
       "12        0.848373   0.704336        0.786120    0.892559         0.817709   \n",
       "13        0.777458   0.776168        0.768034    0.969031         0.822309   \n",
       "14        0.805340   0.598215        0.810003    0.792687         0.859377   \n",
       "15        0.816490   0.597674        0.797584    0.786798         0.826060   \n",
       "16        0.816482   0.495484        0.837049    0.682045         0.860906   \n",
       "17        0.822991   0.837515        0.761707    1.018111         0.806302   \n",
       "18        0.821124   0.567542        0.809420    0.750894         0.834026   \n",
       "19        0.849344   0.559805        0.809790    0.750686         0.838485   \n",
       "20        0.800224   0.854064        0.742851    1.054869         0.786894   \n",
       "\n",
       "    recall_valid  f1_score_valid  fold_valid  \n",
       "1       0.750615        0.651938           1  \n",
       "2       0.697033        0.453930           2  \n",
       "3       0.789725        0.723710           3  \n",
       "4       0.787075        0.646243           4  \n",
       "5       0.775672        0.661167           5  \n",
       "6       0.767156        0.600589           6  \n",
       "7       0.732279        0.647981           7  \n",
       "8       0.803608        0.735090           8  \n",
       "9       0.730189        0.614522           9  \n",
       "10      0.817732        0.739106          10  \n",
       "11      0.810993        0.738429          11  \n",
       "12      0.762636        0.634734          12  \n",
       "13      0.727989        0.605484          13  \n",
       "14      0.770223        0.681377          14  \n",
       "15      0.775156        0.702564          15  \n",
       "16      0.814834        0.759231          16  \n",
       "17      0.732190        0.610393          17  \n",
       "18      0.787257        0.700834          18  \n",
       "19      0.785157        0.716605          19  \n",
       "20      0.708999        0.655696          20  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data - frames: 88.1% +-  1.9%\n",
      "F1_Score on training data - frames: 82.2% +- 3.2%\n",
      "NLL on training data - frames: 0.33 +- 0.05\n",
      "accuracy on validation data - frames: 79.2% +- 2.9%\n",
      "F1-Score on validation data - frames: 65.9% +- 6.2%\n",
      "NLL on validation data - frames: 0.67 +- 0.13\n"
     ]
    }
   ],
   "source": [
    "# learning rate 0.005%\n",
    "mean = results_scores.mean()\n",
    "std = results_scores.std()\n",
    "\n",
    "print(f'accuracy on training data - frames: {mean[\"accuracy_train\"]*100:.1f}% +-  {std[\"accuracy_train\"]*100:.1f}%')\n",
    "print(f'F1_Score on training data - frames: {mean[\"f1_score_train\"]*100:.1f}% +- {std[\"f1_score_train\"]*100:.1f}%')\n",
    "print(f'NLL on training data - frames: {mean[\"NLL_train\"]:.2f} +- {std[\"NLL_train\"]:.2f}')\n",
    "\n",
    "print(f'accuracy on validation data - frames: {mean['accuracy_valid']*100:.1f}% +- {std['accuracy_valid']*100:.1f}%')\n",
    "print(f'F1-Score on validation data - frames: {mean['f1_score_valid']*100:.1f}% +- {std['f1_score_valid']*100:.1f}%')\n",
    "print(f'NLL on validation data - frames: {mean['NLL_valid']:.2f} +- {std['NLL_valid']:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data - frames: 87.4% +-  1.3%\n",
      "F1_Score on training data - frames: 81.3% +- 2.3%\n",
      "NLL on training data - frames: 0.35 +- 0.04\n",
      "accuracy on validation data - frames: 79.5% +- 3.1%\n",
      "F1-Score on validation data - frames: 66.4% +- 7.0%\n",
      "NLL on validation data - frames: 0.65 +- 0.14\n"
     ]
    }
   ],
   "source": [
    "# learning rate 0.001\n",
    "mean = results_scores.mean()\n",
    "std = results_scores.std()\n",
    "\n",
    "print(f'accuracy on training data - frames: {mean[\"accuracy_train\"]*100:.1f}% +-  {std[\"accuracy_train\"]*100:.1f}%')\n",
    "print(f'F1_Score on training data - frames: {mean[\"f1_score_train\"]*100:.1f}% +- {std[\"f1_score_train\"]*100:.1f}%')\n",
    "print(f'NLL on training data - frames: {mean[\"NLL_train\"]:.2f} +- {std[\"NLL_train\"]:.2f}')\n",
    "\n",
    "print(f'accuracy on validation data - frames: {mean['accuracy_valid']*100:.1f}% +- {std['accuracy_valid']*100:.1f}%')\n",
    "print(f'F1-Score on validation data - frames: {mean['f1_score_valid']*100:.1f}% +- {std['f1_score_valid']*100:.1f}%')\n",
    "print(f'NLL on validation data - frames: {mean['NLL_valid']:.2f} +- {std['NLL_valid']:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_35\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_35\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_18      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_and_pad_la… │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReshapeAndPadLaye…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,   │            │                   │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)]               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">976</span> │ reshape_and_pad_… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_104         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_18      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_104[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ reshape_and_pad_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_250 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,240</span> │ concatenate_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_251 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │ conv1d_250[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_64    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_251[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_252 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │ max_pooling1d_64… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_253 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │ conv1d_252[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_65    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_253[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_254 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ max_pooling1d_65… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_255 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ conv1d_254[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_256 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ conv1d_255[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_66    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_256[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_257 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │ max_pooling1d_66… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_257[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_258 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_35          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_258[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_259 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,935</span> │ dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_105         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_259[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ up_sampling2d_43    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_105[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_106         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ up_sampling2d_43… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_260 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">975</span> │ max_pooling1d_65… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_106[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ conv1d_260[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_261 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span> │ add_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_107         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_261[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ up_sampling2d_44    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ reshape_107[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_108         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ up_sampling2d_44… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_18      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m27\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_and_pad_la… │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m146\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ input_layer_18[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mReshapeAndPadLaye…\u001b[0m │ \u001b[38;5;34m5\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m,   │            │                   │\n",
       "│                     │ \u001b[38;5;34m7\u001b[0m)]               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m144\u001b[0m,    │        \u001b[38;5;34m976\u001b[0m │ reshape_and_pad_… │\n",
       "│                     │ \u001b[38;5;34m16\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_104         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_18      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m23\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ reshape_104[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ reshape_and_pad_… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_250 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │      \u001b[38;5;34m2,240\u001b[0m │ concatenate_18[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_251 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │      \u001b[38;5;34m3,104\u001b[0m │ conv1d_250[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_64    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_251[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_252 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │      \u001b[38;5;34m6,208\u001b[0m │ max_pooling1d_64… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_253 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │     \u001b[38;5;34m12,352\u001b[0m │ conv1d_252[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_65    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ conv1d_253[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_254 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m24,704\u001b[0m │ max_pooling1d_65… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_255 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m49,280\u001b[0m │ conv1d_254[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_256 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m49,280\u001b[0m │ conv1d_255[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_66    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_256[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_257 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m295,040\u001b[0m │ max_pooling1d_66… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_257[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_258 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m16,512\u001b[0m │ dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_35          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_258[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_259 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m15\u001b[0m)    │      \u001b[38;5;34m1,935\u001b[0m │ dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_105         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m15\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv1d_259[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ up_sampling2d_43    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m15\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ reshape_105[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mUpSampling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_106         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m15\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ up_sampling2d_43… │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_260 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m15\u001b[0m)    │        \u001b[38;5;34m975\u001b[0m │ max_pooling1d_65… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_26 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m15\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ reshape_106[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ conv1d_260[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_261 (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m15\u001b[0m)    │        \u001b[38;5;34m240\u001b[0m │ add_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_107         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m15\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv1d_261[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ up_sampling2d_44    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m1\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ reshape_107[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mUpSampling2D\u001b[0m)      │ \u001b[38;5;34m15\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape_108         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m, \u001b[38;5;34m15\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ up_sampling2d_44… │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,388,540</span> (5.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,388,540\u001b[0m (5.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">462,846</span> (1.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m462,846\u001b[0m (1.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">925,694</span> (3.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m925,694\u001b[0m (3.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mdl13.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.12.2 64bit [MSC v.1916 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "8.25.0"
        },
        {
         "module": "OS",
         "version": "Windows 10 10.0.19045 SP0"
        },
        {
         "module": "tensorflow",
         "version": "2.16.1"
        },
        {
         "module": "numpy",
         "version": "1.26.4"
        },
        {
         "module": "matplotlib",
         "version": "3.8.0"
        },
        {
         "module": "plotly",
         "version": "5.9.0"
        },
        {
         "module": "pandas",
         "version": "2.2.1"
        },
        {
         "module": "keras",
         "version": "3.1.1"
        },
        {
         "module": "seaborn",
         "version": "0.13.2"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.12.2 64bit [MSC v.1916 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>8.25.0</td></tr><tr><td>OS</td><td>Windows 10 10.0.19045 SP0</td></tr><tr><td>tensorflow</td><td>2.16.1</td></tr><tr><td>numpy</td><td>1.26.4</td></tr><tr><td>matplotlib</td><td>3.8.0</td></tr><tr><td>plotly</td><td>5.9.0</td></tr><tr><td>pandas</td><td>2.2.1</td></tr><tr><td>keras</td><td>3.1.1</td></tr><tr><td>seaborn</td><td>0.13.2</td></tr><tr><td colspan='2'>Mon Jul 15 08:22:39 2024 Mitteleuropäische Sommerzeit</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.12.2 64bit [MSC v.1916 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 8.25.0 \\\\ \\hline\n",
       "OS & Windows 10 10.0.19045 SP0 \\\\ \\hline\n",
       "tensorflow & 2.16.1 \\\\ \\hline\n",
       "numpy & 1.26.4 \\\\ \\hline\n",
       "matplotlib & 3.8.0 \\\\ \\hline\n",
       "plotly & 5.9.0 \\\\ \\hline\n",
       "pandas & 2.2.1 \\\\ \\hline\n",
       "keras & 3.1.1 \\\\ \\hline\n",
       "seaborn & 0.13.2 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Mon Jul 15 08:22:39 2024 Mitteleuropäische Sommerzeit} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.12.2 64bit [MSC v.1916 64 bit (AMD64)]\n",
       "IPython 8.25.0\n",
       "OS Windows 10 10.0.19045 SP0\n",
       "tensorflow 2.16.1\n",
       "numpy 1.26.4\n",
       "matplotlib 3.8.0\n",
       "plotly 5.9.0\n",
       "pandas 2.2.1\n",
       "keras 3.1.1\n",
       "seaborn 0.13.2\n",
       "Mon Jul 15 08:22:39 2024 Mitteleuropäische Sommerzeit"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%version_information tensorflow, numpy, matplotlib, plotly, pandas, keras, seaborn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gestclass2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
